%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter4.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter4.tex}%

\chapter{Proposed Solution}
\label{cha:proposed-solution}

\epigraph{
  This chapter provides a detailed overview of the proposed system and briefly exposes the solution's validation methodology. It also addresses the expected technological stack and the envisioned plan for the system's development. The work schedule is split into five distinct and concisely explained tasks mapped in a Gantt chart.
}{}

\section{Proposal Overview}

The solution we propose consists of a user-friendly \gls{3D} soundscape editor for Windows, specifically designed to support the creation and functionality of immersive virtual auditory environments. These environments will be made available and interactive through a BVI-accessible mobile application, given the widespread use of smartphones. With this approach, we intend to enable curators to generate immersive \gls{3D} audio environments representative of specific paintings, and to have \gls{BVI} users explore these environments interactively. Figure~\ref{fig:concept-img} attempts to illustrate the intended approach from a purely conceptual standpoint.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{concept-cropped}
  \caption{Concept representation. Image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado}).}
  \label{fig:concept-img}
\end{figure}

Thus, the system we aim to develop has two very different symbiotic parts. The first is the de facto editing of \gls{3D} sound environments in a desktop application, catering to curators without previous experience in sound engineering and related software. It abstracts the complexities of spatializing sound sources and optimizing accessibility, automatically generating BVI-accessible environments upon exportation. Furthermore, it features two toggleable modes, one for development and the second for testing. Just as important, the other part consists of a mobile application where the previously generated scenes are made available for interaction. It concerns the interactivity, functionality, and accessibility of said environments for \gls{BVI} individuals, on a mobile device.

\subsection{Mobile Soundscape Player}

The mobile application's interface (portrayed in figure~\ref{fig:mobile-overview-interface}) displays a top-down \gls{2D} representation of a soundscape, with large and high-contrasting elements representing directional sound sources, making them easier to distinguish within the environment. This approach makes use of standard \gls{BVI} accessibility practices for visual interfaces in other research~\cite{ahmetovic2021musa,drossos2015accessible,simao2018jogo}. Ambient sound sources are also present but permanently hidden to prevent users with partial sight from misinterpreting them for localized sounds. The user’s head and ears are represented in the scene by a small arrow that always faces upwards on the screen, regardless of the user’s orientation within the virtual space.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{goku}
  \caption{Concept representation. Image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado}).}
  \label{fig:mobile-overview-interface}
\end{figure}

Still concerning the visual interface, the application offers three different display modes for users with residual vision (refer to figure X), and since these are purely visual, alternating between them is executed via a visual button in the upper left corner of the screen:
\begin{itemize}
  \item Immersive Mode (default): Displays the background painting and the sound sources.
  \item Accessible Mode: A pure white background with further contrasting sound sources, where virtual joysticks are darker for better contrast.
  \item Minimalist Mode: Displays the background painting but hides the sound sources, with the virtual joysticks made more transparent for minimal distraction.
\end{itemize}

So that users can properly detect the directionality and proximity of sound sources through binaural cues, the use of headphones is required for an accurate perception of spatialized sounds.

Navigation through the virtual space is enabled via a joystick control scheme, inspired by Nair et al.’s~\cite{nair2022uncovering} Dungeon Escape game and other classic \gls{3D} games such as the early Resident Evil\footnote{https://game.capcom.com/residentevil/en/} titles. The application supports both gamepad and touch controls, with these two schemes resembling each other as much as possible. For example, most directional buttons on the gamepad trigger the same actions as swipe gestures on the touch interface. When a gamepad controller is connected to the smartphone, the virtual joysticks are automatically hidden and disabled, returning to active if the controller is disconnected.

The left joystick controls the user’s movement within the painting: vertical tilts move the user forward or backward, and horizontal tilts execute snap rotations in fixed increments of 30 degrees, complemented by distinct audio cues. If the user reaches the border of the painting, movement is halted in the direction of the boundary. In addition, all sounds are muted and the smartphone emits a strong vibration. Upon returning to the in-bounds area by moving away from the boundary, the sounds resume and so does normal movement.

The right joystick acts as a directional scanner akin to to Nair et al.'s~\cite{nair2021navstick} NavStick, though in \gls{2D} rather than in \gls{3D}. By tilting the joystick in a particular direction, the user can hear a localized narration of the first object in their line of sight, through a predefined speech output. Figure~\ref{fig:mobile-overview-joystick} exemplifies this scanner in practical use. To assist with a more literal interpretation of the environment, a global verbal description of the entire scene can be triggered, during which all spatial sounds pause and then resume once the narration finishes. The idea of using blocking sounds (narrations) and interruptible sounds (spatial sounds in this particular case) was inspired by Drossos et al.'s~\cite{drossos2015accessible} implementation of a BVI-accessible Tic-Tac-Toe game.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{goku}
  \caption{Concept representation. Image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado}).}
  \label{fig:mobile-overview-joystick}
\end{figure}

An additional, no less important \gls{SAT} is available: a proximity detection tool, inspired by the rear sensors in modern vehicles, which signal for nearby collidable obstacles. Much in the same manner, this toggleable proximity sensor emits an intermittent sound that increases in frequency as the user approaches an object (a spatial sound source in this context). Contrasting that of the rear of a car, the implemented sensor uses a forward cone of detection, identifying the closest spatial sound source within range. There are three range levels: far, intermediate, and close. Each corresponds to progressively more frequent and louder localized beeps, as the user approaches the detected sound source.

To regulate sensory and cognitive overload, a significant concern for \gls{BVI} individuals~\cite{guerreiro2023design,nair2022uncovering}, users may control the density of sensory inputs to a degree. One such example is the ability to toggle ambient sound on or off. Additionally, influenced by Guerreiro et al.'s~\cite{guerreiro2023design} framework to support BVI-inclusive auditory representations, users can control the number of active spatial sound sources at any given moment. To further mitigate cognitive overload and since Guerreiro et al's research stated that fully concurrent auditory feedback could be mentally overwhelming for BVI individuals, the application offers two spatial sound reproduction modes: simultaneous and sequential. As the name implies, in sequential mode, all spatial sounds play concurrently. In sequential mode, sounds are consecutively played by order of proximity, lasting for a fixed amount of time. Once they have all played, the reproduction cycles starts over, with the order updated based on the user's position at the time.


\subsection{Soundscape Editor for Windows}

Inspired by Ferreira’s~\cite{ferreira2021creating} work on Immerscape (addressed in section~\ref{ssec:immerscape}), the editing tool to be developed will feature predefined environments of varying acoustic properties, such as resonance and reverberation, which may be further customized. 

It is convenient for users to easily manipulate and experiment with different sound elements, such as Minecraft\footnote{https://www.minecraft.net/en-us}’s block-based system, where users can place or erase different types of blocks with distinct functions. Thus, the scene is built with a similar block-based system and, like Minecraft’s creative mode, allows for movement in all directions, including free flight. There will be multiple block types, such as \gls{3D} sound emitter, ambient audio, collidable block (only simulates physical barriers, does not emit sound), and user-customized ones. These blocks are editable in several ways, some of which are volume, loop settings, sound file association, and object representation.

When developing a scene, insights into the end user’s experience are crucial to iteratively improving it. Hence, the editor will have two main modes: Development Mode, where scenes are built and tailored, and Interactive Mode, where users can test the experience from the end user's point of view, though with desktop controls.

Additionally, the user may define default values and boundaries for movement speed, audio cardinality, and zoom level in the exported scene and verbally describe the whole piece. Once a scene is created, it can be saved, loaded, and edited. Furthermore, a built-in help function will always be available, offering a concise explanation of the system’s functionalities. Users will supply their sound assets, storing them in a dedicated directory in supported formats (these formats are yet to be decided but are likely to be WAV and/or MP3). On exportation, the editor will package the scene's data (possibly in JSON or XML) and store it in a designated folder.


\section{Technological Stack}

We selected the Unity game engine as the primary development environment for the desktop soundscape editor and mobile application prototype. One factor that solidified this choice was its adoption among some of the most relevant related work we have analyzed~\cite{ferreira2021creating,guerreiro2023design,nair2021navstick,nair2022uncovering,simao2018jogo,yang2019audio} in chapter~\ref{cha:related-work}.

A real-time \gls{3D} development platform, while Unity is mainly known for its role in game development, its versatility has led to widespread use in all sorts of interactive \gls{2D} and \gls{3D} experiences across various industries~\cite{techical-overview-unity,what-is-unity}. It provides extensive cross-platform build support, including the hardware we are particularly interested in: Windows PC (for the editor application) and Android (for the mobile interaction).

Aside from its powerful rendering capabilities, Unity sports strong community support, a vast amount of learning resources, and a convenient package manager extending its functionality with third-party libraries. Its learning curve is also friendlier than other mainstream \gls{3D} game engines like Unreal Engine.

With a built-in \gls{3D} audio system, Unity provides a solid foundation for basic sound spatialization with distance attenuation according to positioning, a spatial blend parameter, and adjustable volume roll of, among some other settings. While it may be adequate for rudimentary \gls{3D} audio, this built-in system is limited, lacking binaural rendering and other relevant capabilities. Fortunately, specialized plugins for advanced spatial audio address the vanilla system’s limitations by adding some of the essential and advanced spatialization functionalities it lacks.

Initially, we had considered the Google Resonance Audio SDK for integration within Unity, as it offered relevant features such as \gls{HRTF} processing and environmental modeling. Furthermore, some of the analyzed studies~\cite{ferreira2021creating,simao2018jogo,yang2019audio} had successfully incorporated this library in their Unity-based environment. However, it has been deprecated for quite some time and has no support for recent versions of the Unity engine.

For these reasons, we turned to the Steam Audio framework instead, which has been actively maintained, well-documented, and compatible with Unity engine updates. Moreover, we were motivated by the use of this toolkit in NavStick~\cite{nair2021navstick} and Dungeon Escape~\cite{nair2022uncovering}, two of the most influential works for our proposal. Supposedly easy to implement and deploy, Steam Audio encompasses high-fidelity HRTF-based binaural rendering and geometry-based occlusion, reflection, and reverberation effects, among other spatial features contributing to natural sounding immersion. While the specific library to which its default \gls{HRTF} dataset belongs is not explicitly stated in official documentation, some online discussions suggest it may be based on Phonon\footnote{https://www.impulsonic.com/what-is-trueaudio-phonon/}.


\section{Implementation}

\subsection{Mobile BVI-Acessible Soundscape Player Prototype}

\lipsum[1-5]

\subsection{Desktop Beginner-Friendly Soundscape Editor}

\lipsum[1-5]

\section{Initial Research \& Concept Refinement}

Prior to the system's technical implementation, conceptual designs of the mobile soundscape player and the desktop editor were produced in Figma, illustrating the key functionalities envisioned for each application. At this stage, the mobile application was identified as the main component to prove the concept, since it would directly serve BVI users.

The first implementation efforts therefore focused on an early version of the mobile prototype, already including most of the features described in section 4.1.1. This build was sufficient to demonstrate the feasibility of the proposed concept for accessible and interactive interpretation of a painting, as was ultimately intended. However, compared to the final application it lacked ambient sound regulation, the proximity sensor, and haptic feedback, among several other adjustments - as these hadn't yet been planned at the time. Along with the mockups of the desktop editor, some of the mobile functionalities were presented in a remote meeting with researchers from the Department of Conservation and Restoration of NOVA School of Science and Technology. The researchers saw potential in the idea and expressed interest in its evolution.

The most important stage of this early phase came in late May, when the mobile prototype was tested on-site at the Raquel e Martin Sain Foundation - a social solidarity institution dedicated to the professional education and social integration of BVI individuals. Two blind participants volunteered their time: Cláudia Pires, a student at the foundation, and Ana Inês Colares, one of its instructors. Both of these interviews followed a simplified protocol with similarities to that of the final study but with less specific tasks and more open-ended questions, for the goal was refinement rather than evaluation. The painting "O Bosque Sagrado" by Maria Benamor was used as the painting for the test scenario, also being used in the final study of the application (section~\ref{sec:mobile-sonic-painting-exploration}).

The first interviewee, Cláudia Pires, pointed to several areas for improvement in the experience. She felt that ambient sound should be lower and that the sound effects used for movement and rotation were too similar, sometimes hard to distinguish. Though she could discern sound direction clearly, perceiving proximity to the sound sources was more difficult. While she found the directional scanner useful, she noted that it required alignment that was much too precise, with narrations cutting off if not perfectly targeted. Cláudia also reported little difference between sequential and simultaneous reproduction modes and sometimes had difficulty tracking how many sounds were active. The clock hour orientation feature was very helpful to her but she stated that it should have been introduced earlier in the protocol. Finally, she strongly preferred the console scheme over the touch gestures, suggesting haptic feedback and larger virtual joysticks as possible improvements to touch usability.

Many of the points brought up by Cláudia were echoed by the second interviewee, Ana Inês Colares, who also provided further insights. Accentuating the importance of the clock hour orientation system, she too felt that this feature should be introduced in an earlier task. Ana Inês suggested that sound sources should increase in volume more noticeably when approached and that ambient sound should be toggleable, as it could distract from the other elements at times. Like Cláudia, she noticed little difference between the two reproduction modes and found sound source direction easier to perceive than proximity, occasionally struggling to locate specific sounds in the environment. She also preferred the console controls and noted the difficulty of touch gestures and especially virtual joysticks without haptic feedback. Lastly, she proposed the addition of a menu to allow switching between different soundscapes, if the application were to support several painting representations. Most importantly, from the conversation with Ana Inês and her emphasis on the amplification of sound according to proximity, derived the idea for the currently implemented proximity sensor.

The purpose of this early study was to garner real input from the individuals to whom the application was intended, and in this way assess whether the proposed features were indeed adequate for BVI needs, rather than only in theory. These preliminary interviews were fundamental in shaping the application into what it eventually became, as both participants provided detailed feedback which revealed not only the prototype's shortcomings but also some of its strengths. As much as possible, their suggestions were incorporated into the application's final design and in the later evaluation (section~\ref{ssec:mobile-results-overview}) these proved both practical and meaningful.
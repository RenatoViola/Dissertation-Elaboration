\typeout{NT FILE chapter4.tex}%

\chapter{Proposed Solution}
\label{cha:proposed-solution}

\epigraph{
  This chapter provides a detailed overview of the proposed system, introducing the BVI-inclusive mobile soundscape player and the soundscape editor for Windows, as well as the system's limitations. An exploration of the initial research and refinement of the proposed concept then follows and the technological stack chosen for the solution's implementation is addressed. A discussion on the implementation of both prototypes composing the system concludes the chapter.
}{}

\section{Proposal Overview}
\label{sec:proposal-overview}

The solution proposed in this dissertation is a bipartite system intended to, above all, facilitate BVI-inclusive interpretation of visual paintings, via immersive and highly interactive soundscapes based on museum paintings. While clearly distinct in purpose and also in the audience they serve, these two allied parts provide both soundscape interaction and intuitive creation. Figure~\ref{fig:concept-img} attempts to illustrate the intended approach from a purely conceptual standpoint, while figure~\ref{fig:system-architecture} displays a high-level view of the system's architecture. The solution's most relevant shortcomings are addressed in section~\ref{sec:system-limitations}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{concept-cropped}
  \caption{Representation of the proposed interaction. Background image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado}).}
  \label{fig:concept-img}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{system-architecture}
  \caption{Conceptual system architecture. \textit{Disclaimer: The current editor prototype does not currently support exportation of the devised scenes, though it was initially intended.}}
  \label{fig:system-architecture}
\end{figure}

The first and ultimately core component is a mobile application, which serves as an interactive and accessible soundscape player for \gls{BVI} visitors. Through its available \gls{SAT}s, the application serves as a vehicle for users to autonomously navigate the artwork and learn about it or even feel it, via various sound effects (be it ambience or spatialized sound), verbal descriptions and customizable sound settings, among other considerations.

The second component is a desktop soundscape editor that enables intuitive creation of these auditory environments, from design to testing, and eventually refinement. This tool targets curators whom are not required to have any prior experience in sound design or programming, abstracting the complexities of spatializing sound sources and optimizing for accessibility, by way of a straightforward interface. While the editor plays a essential role in the system, it fundamentally serves as a supplement to the mobile application - as the means to produce the content available within it.

The following two sections detail these components further, starting by the mobile constituent (section~\ref{ssec:mobile-soundscape-player-overview}) and concluding with the desktop editor (section~\ref{ssec:soundscape-editor-overview}).

\subsection{Mobile Soundscape Player}
\label{ssec:mobile-soundscape-player-overview}

The mobile application's interface (portrayed in figure~\ref{fig:mobile-overview-interface}) displays a \gls{2D} representation of a soundscape, with large and high-contrasting icons representing directional sound sources, making them easier to distinguish within the environment. This approach makes use of standard \gls{BVI} accessibility practices for visual interfaces in other research~\cite{ahmetovic2021musa,drossos2015accessible,simao2018jogo} with spatial sounds assigned to points of interest in the painting~\cite{yang2019audio}. Ambient sound sources are also present but their icons are permanently hidden to prevent users with partial sight from misinterpreting them for localized sound sources associated to elements of the actual painting. The user's head and ears are represented in the scene by a small arrow that always faces upwards on the screen, regardless of the user's orientation within the virtual space, as in a typical videogame minimap.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{mobile-display-modes}
  \caption{The three available display modes of the mobile prototype: Immersive, Accessible and Minimalist.}
  \label{fig:mobile-overview-interface}
\end{figure}

Still concerning the visual interface, the application offers three different display modes for users with residual vision (refer to figure~\ref{fig:mobile-overview-interface}), and since these are purely visual, alternating between them is executed via a visual button in the upper left corner of the screen:
\begin{itemize}
  \item \textbf{Immersive mode (default)} Displays the background painting and the sound sources.
  \item \textbf{Accessible mode:} A pure white background with further contrasting sound sources, where virtual joysticks are darker for better contrast.
  \item \textbf{Minimalist mode:} Displays the background painting but hides the sound sources, with the virtual joysticks made more transparent for minimal distraction.
\end{itemize}

So that users can properly detect the directionality and proximity of sound sources through binaural cues, the use of headphones is required for an accurate perception of spatialized sounds.

Navigation through the virtual space is enabled via a joystick control scheme, inspired by Nair et al.’s~\cite{nair2022uncovering} Dungeon Escape game and other classic \gls{3D} games such as the early Resident Evil\footnote{\url{https://game.capcom.com/residentevil/en/}} titles. The application supports both gamepad and touch controls, with these two schemes resembling each other as much as possible. For example, most directional buttons on the gamepad trigger the same actions as swipe gestures on the touch interface. When a gamepad controller is connected to the smartphone, the virtual joysticks are automatically hidden and disabled, returning to active if the controller is disconnected.

The left joystick controls the user’s movement within the painting: vertical tilts move the user forward or backward, and horizontal tilts execute snap rotations in fixed increments of 30 degrees, complemented by distinct audio cues. If the user reaches the border of the painting, movement is halted in the direction of the boundary. In addition, all sounds are muted and the smartphone emits a strong vibration. Upon returning to the in-bounds area by moving away from the boundary, the sounds resume and so does normal movement.

The right joystick acts as a directional scanner akin to to Nair et al.'s~\cite{nair2021navstick} NavStick, though in \gls{2D} rather than in \gls{3D}. By tilting the joystick in a particular direction, the user can hear a localized narration of the first object in their line of sight, through a predefined speech output. Figure~\ref{fig:mobile-overview-joystick} exemplifies this scanner in practical use. To assist with a more literal interpretation of the environment, a global verbal description of the entire scene can be triggered, during which all spatial sounds pause and then resume once the narration finishes. The idea of using blocking sounds (narrations) and interruptible sounds (spatial sounds in this particular case) was inspired by Drossos et al.'s~\cite{drossos2015accessible} research on a BVI-accessible Tic-Tac-Toe game.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{scanner-detection}
  \caption{Directional scanner in use: on the left the scanner has not detected an element while on the right it has detected a spatial sound source and narrates it.}
  \label{fig:mobile-overview-joystick}
\end{figure}

An additional, no less important \gls{SAT} is available: a proximity detection tool, inspired by the rear sensors in modern vehicles, which signal for nearby collidable obstacles. Much in the same manner, this toggleable proximity sensor emits an intermittent sound that increases in frequency as the user approaches an object (a spatial sound source in this context). Contrasting that of the rear of a car, the implemented sensor uses a forward cone of detection, identifying the closest spatial sound source within range. There are three range levels: far, intermediate, and close. Each corresponds to progressively more frequent and louder localized beeps, as the user approaches the detected sound source.

To regulate sensory and cognitive overload, a significant concern for \gls{BVI} individuals~\cite{guerreiro2023design,nair2022uncovering}, users may control the density of sensory inputs to a degree. One such example is the ability to toggle ambient sound on or off. Additionally, influenced by Guerreiro et al.'s~\cite{guerreiro2023design} framework to support BVI-inclusive auditory representations, users can control the number of active spatial sound sources at any given moment. To further mitigate cognitive overload and since Guerreiro et al.'s research stated that fully concurrent auditory feedback could be mentally overwhelming for \gls{BVI} individuals, the application offers two spatial sound reproduction modes: simultaneous and sequential. As the name implies, in simultaneous mode, all spatial sounds play concurrently. In sequential mode, sounds are consecutively played by order of proximity, lasting for a fixed amount of time. Once they have all played, the reproduction cycle starts over, with the order updated based on the user's position at the time.


\subsection{Soundscape Editor for Windows}
\label{ssec:soundscape-editor-overview}

Drawing inspiration from Ferreira’s~\cite{ferreira2021creating} Immerscape, this editor aims to serve as an intuitive soundscape creation tool for curators or artists without prior expertise in sound design or programming, enabling them to translate visual paintings into the BVI-inclusive auditory environments present in the mobile application discussed in section~\ref{ssec:mobile-soundscape-player-overview}.

The editor is designed to minimize complexity for curators by enabling interaction primarily through the mouse. All file management, such as choosing a painting image and selecting audio files is handled by means of an versatile file browser which allows users to organize and fetch media resources from wherever in the desktop they see fit.

Initially, the application was meant to be much more similar to Immerscape than it currently is, featuring a \gls{3D} development environment with predefined acoustic properties like reverberation and resonance, alongside a block-based placement system similar to Minecraft\footnote{\url{https://www.minecraft.net/en-us}}’s creative mode. However, during the mockup elaboration stage (mockups can be viewed in section~\ref{app:mockup-desktop}), it was conjectured that in the context of \gls{BVI} accessibility, introducing additional complexity to the scene in the form of acoustic properties, like reverberation and resonance, could bring about uneccessary complexity and cognitive overload. Most importantly, enforcing a \gls{3D} development environment could be cumbersome and time-consuming for curators, who would be required to interpret and translate a \gls{2D} painting into a \gls{3D} auditory environment faithfully embodying the artwork. As a result, the decision was made to simplify the development system and have it be in \gls{2D} with a grid-based placement system in place, reducing cognitive strain while retaining the two-dimensional essence of the original painting.

The workflow within the application is divided into two main modes: Development mode and Interactive mode. As a quick introduction, it is in Development mode that curators may design the soundscape, while Interactive Mode provides a built-in interactive preview of the devised soundscape, simulating how it would be experienced by the end user in the mobile application.

In Development mode (exposed in figure~\ref{fig:desktop-editor-development-mode}), users begin by selecting a painting (via the file browser), which is displayed as a background image in the scene. Zoom level is automatically adjusted to ensure the image appropriately fits the screen, and the user may further zoom in or out to taste. Setting a global narration to represent the artwork is executed identically to defining the background image. Placement, editing and removal of elements in the scene is done via a simple grid-based building system reminiscent of city builder games, but in \gls{2D}. This rectangular grid layout dinamically fits to the painting's dimensions and is customizable in both the size of its cells and the thickness of its lines, overlaying the painting when enabled.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{development-mode}
  \caption{Development mode. Background image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado})}
  \label{fig:desktop-editor-development-mode}
\end{figure}
With the painting set, curators may place elements in the grid, of which there are three clearly distinct types, selectable from the panel at the bottom center of the screen:
\begin{itemize}
  \item \textbf{Ambient Sound:} Represents general background noise and ambience whose effect is independent of spatial placement within the grid. Its editable attributes are: \textit{name, audio file \& volume, looping, fade-in and fade-out};
  \item \textbf{Spatial Sound:} Represents a fully localized sound effect, playing only from the specific position in which it was placed. Attributes include: \textit{all attributes from ambient sound, narration audio file \& correspondent volume and volume rolloff settings, to simulate how sound intensity decreases with distance};
  \item \textbf{Player:} Represents the person exploring the scene and acts as the controller for movement and hearing within it. This element's position in the grid determines the end user's initial position. Its customizable attributes are: \textit{initial clockhour orientation, movement speed and proximity sensor settings, in which the amplitude of the detection cone is included}.
\end{itemize}

The grid remains visible only when adjusting its properties or when active for placing, editing, or removing elements and can be closed via a button in the lower left corner, which only appears when the grid is enabled. Placing an element consists only on having it actively selected and clicking on an empty grid cell to place it, and no cell can have more than one element assigned to it. Editing and removing elements work function identically except that a cell must already have an element to be valid to act upon. When editing an element (see figure~\ref{fig:desktop-editor-editing-popups}), changes are persisted automatically rather than via an explicit submit button. Additionally, the radiuses of volume rollof in spatial sound elements and proximity definition in the player element's sensor can be viewed in the grid layout via overlapping circles of decreasing opacity, as can be seen in figure~\ref{fig:desktop-editor-sensor-radius}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{sound-popups}
  \caption{Editing popup windows for a ambient sound element on the left, and a spatial sound element on the right.}
  \label{fig:desktop-editor-editing-popups}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{sensor-radius}
  \caption{Editing popup of the player element on the left. The rightmost image allows the customization of the proximity sensor's reach and varying frequency areas.}
  \label{fig:desktop-editor-sensor-radius}
\end{figure}

When users want to preview the soundscape they devised as it will be made available to the visitor audience, they should use headphones for accurate spatial sound perception and enter Interactive mode (figure~\ref{fig:desktop-editor-interactive-mode}). This mode effectively emulates the mobile application covered in section~\ref{ssec:mobile-soundscape-player-overview}: they may move around and scan the scene with the virtual joysticks in the lower corners of the screen and use every functionality available in the mobile application, but trigger them through buttons rather than touch gestures. At any time, users may return to Development mode, easily alternating between the two.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{interactive-mode}
  \caption{Interactive mode.}
  \label{fig:desktop-editor-interactive-mode}
\end{figure}


\section{System Limitations}
\label{sec:system-limitations}

Due to time constraints, some initially planned features were not implemented in the desktop editor's prototype. More specifically, the editor lacks the ability to export the devised scenes to the mobile soundscape player and is thus incapable of providing content to the mobile application as was intended. 

Additionally, although there is a simple local save system in place, there was not enough time to build the appropriate interface elements to expose it to an end user. As such, the editor does not currently support a proper save or load feature for the generated environments. To account for the possibility of users testing the application remotely on their desktops, scene data generated and persisted during the application's runtime is automatically deleted when it closes to avoid leaving residual files on the user’s system. 

The mobile soundscape player shares its parser with the editor's Interactive mode, working exclusively with local files (fetching media resources by file path), limiting the potential for future integration with available online scenes generated by the editor. 

As it stands, both prototypes fall short of a fully functional soundscape player or editor, as initially envisioned. Despite these limitations, they are competent and useful tools for validating the core concepts of the proposed system. Section~\ref{sec:future-work} delves into how these features may be addressed in a future iteration of the system.


\section{Initial Research \& Concept Refinement}
\label{sec:initial-research}

Prior to the system's technical implementation, conceptual designs of the mobile soundscape player and the desktop editor were produced in Figma\footnote{\url{https://www.figma.com/}}, illustrating the key functionalities envisioned for each application (these mockups are available in appendix~\ref{app:conceptual-mockups}). At this stage, the mobile application was identified as the main component to prove the concept, since it would directly serve \gls{BVI} users.

The first implementation efforts therefore focused on an early version of the mobile prototype, already including most of the features described in section~\ref{ssec:mobile-soundscape-player-overview}. This build was sufficient to demonstrate the feasibility of the proposed concept for accessible and interactive interpretation of a painting, as was ultimately intended. However, compared to the final application it lacked ambient sound regulation, the proximity sensor, and haptic feedback, among several other adjustments - as these hadn't yet been planned at the time. Along with the mockups of the desktop editor, some of the mobile functionalities were presented in a remote meeting with researchers from the Department of Conservation and Restoration of NOVA School of Science and Technology. The researchers saw potential in the idea and expressed interest in its evolution.

The most important stage of this early phase came in late May, when the mobile prototype was tested on-site at the Raquel e Martin Sain Foundation\footnote{\url{https://www.fundacao-sain.pt/}} - a social solidarity institution dedicated to the professional education and social integration of \gls{BVI} individuals. Two blind participants volunteered their time: Cláudia Pires, a student at the foundation, and Ana Inês Colares, one of its instructors. Both of these interviews followed a simplified protocol with similarities to that of the final study but with less specific tasks and more open-ended questions, for the goal was refinement rather than evaluation. The painting "O Bosque Sagrado"~\cite{o-bosque-sagrado} by Maria Benamor was used as the painting for the test scenario, also being used in the final study of the application (section~\ref{sec:mobile-sonic-painting-exploration}).

The first interviewee, Cláudia Pires, pointed to several areas for improvement in the experience. She felt that ambient sound should be lower and that the sound effects used for movement and rotation were too similar, sometimes hard to distinguish. Though she could discern sound direction clearly, perceiving proximity to the sound sources was more difficult. While she found the directional scanner useful, she noted that it required alignment that was much too precise, with narrations cutting off if not perfectly targeted. Cláudia also reported little difference between sequential and simultaneous reproduction modes and sometimes had difficulty tracking how many sounds were active. The clock hour orientation feature was very helpful to her but she stated that it should have been introduced earlier in the protocol. Finally, she strongly preferred the console scheme over the touch gestures, suggesting haptic feedback and larger virtual joysticks as possible improvements to touch usability.

Many of the points brought up by Cláudia were echoed by the second interviewee, Ana Inês Colares, who also provided further insights. Accentuating the importance of the clock hour orientation system, she too felt that this feature should be introduced in an earlier task. Ana Inês suggested that sound sources should increase in volume more noticeably when approached and that ambient sound should be toggleable, as it could distract from the other elements at times. Like Cláudia, she noticed little difference between the two reproduction modes and found sound source direction easier to perceive than proximity, occasionally struggling to locate specific sounds in the environment. She also preferred the console controls and noted the difficulty of touch gestures and especially virtual joysticks without haptic feedback. Lastly, she proposed the addition of a menu to allow switching between different soundscapes, if the application were to support several painting representations. Most importantly, from the conversation with Ana Inês and her emphasis on the amplification of sound according to proximity, derived the idea for the currently implemented proximity sensor.

The purpose of this early study was to garner real input from the individuals to whom the application was intended, and in this way assess whether the proposed features were indeed adequate for \gls{BVI} needs, rather than only in theory. These preliminary interviews were fundamental in shaping the application into what it eventually became, as both participants provided detailed feedback which revealed not only the prototype's shortcomings but also some of its strengths. As much as possible, their suggestions were incorporated into the application's final design and in the later evaluation (section~\ref{ssec:mobile-results-overview}) these proved both practical and meaningful.


\section{Technological Stack}
\label{sec:technological-stack}

The Unity\footnote{\url{https://unity.com/}} game engine, namely Unity 6, was selected as the primary development environment for the desktop soundscape editor and mobile application prototype. The main factor that solidified this choice was its adoption among some of the most relevant related work~\cite{ferreira2021creating,guerreiro2023design,nair2021navstick,nair2022uncovering,simao2018jogo,yang2019audio} analyzed in chapter~\ref{cha:related-work}.

While Unity is mainly known for its role in game development, its versatility has led to widespread use in all sorts of interactive \gls{2D} and \gls{3D} experiences across various industries~\cite{techical-overview-unity,what-is-unity}. Most importantly, it provides extensive cross-platform build support, including the hardware this dissertation concerns itself with: Windows (for the editor application) and Android (for the mobile interaction).

Aside from its powerful rendering capabilities, Unity has a friendlier learning curve than other mainstream \gls{3D} game engines like Unreal Engine\footnote{\url{https://www.unrealengine.com/}} and features a convenient package manager extending its functionality with third-party libraries.

Unity's built-in audio system already provides sturdy audio management, featuring distance attenuation, spatial blend, and adjustable volume rolloff, among other settings. However, while this system suffices for rudimentary \gls{3D} audio, it is limited for the purposes of this work, lacking binaural rendering and other relevant capabilities. There are however specialized plugins for advanced spatial audio that address the vanilla system’s limitations by adding some of the essential and advanced spatialization functionalities it lacks.

The Steam Audio\footnote{\url{https://valvesoftware.github.io/steam-audio/}} framework was selected to address this base engine limitation and has been actively maintained, well-documented, and compatible with Unity engine updates. Furthermore, this toolkit was also used in NavStick~\cite{nair2021navstick} and Dungeon Escape~\cite{nair2022uncovering}, two of the most influential works for this proposal, motivating its adoption. Steam Audio supports high-fidelity \gls{HRTF}-based binaural rendering and geometry-based occlusion, reflection, and reverberation effects, among other spatial features contributing to natural sounding immersion. Though out of these features, only binaural nearest interpolation rendering and curve drive distance attenuation were employed (with the built-in Steam Audio \gls{HRTF}) and other properties retained their default values. While the specific library to which its default \gls{HRTF} dataset belongs is not explicitly stated in official documentation, some online discussions suggest it may be based on Phonon\footnote{\url{https://www.impulsonic.com/what-is-trueaudio-phonon/}}, however this remains an assumption.

Aside from Steam Audio, several external packages were integrated within the Unity environment in order to agilize development. One such example is DOTween\footnote{\url{https://assetstore.unity.com/packages/tools/animation/dotween-hotween-v2-27676}}, an animation engine employed in both the mobile and desktop components of the system, for smooth transitions visually and also in sound (namely the fade effects). For haptic feedback on the smartphone, the HapticFeedback\footnote{\url{https://github.com/CandyCoded/HapticFeedback}} library was used, supporting vibration patterns of varying intensities. File system interactions in the desktop editor such as opening a file from a local desktop directory via a file dialog were supported by the UnityStandaloneFileBrowser\footnote{\url{https://github.com/gkngkc/UnityStandaloneFileBrowser}}, a third-party tool alongside the above mentioned HapticFeedback library. Lastly and also the most essential, the Input System Package\footnote{\url{https://docs.unity3d.com/Packages/com.unity.inputsystem@1.14/manual/index.html}} was incorporated to handle all input, from keyboard bindings, to gamepad controls and even the full implementation of the touch interface, effectively replacing the entirety of Unity's built-in input system.

Apart from Unity and the C\# programming language in which its scripts are written, the Python language was used for a single script (outside of the system itself) which interacted with the Google Cloud Text-to-Speech API\footnote{\url{https://console.cloud.google.com/marketplace/product/google/texttospeech.googleapis.com}}. The intent of this script was to generate narrations for functionalities such as clock hours for the orientation feature and activation/deactivation sound effects. Conveniently, it was also used to generate the consistent verbal descriptions utilized in the prototypes presented to end users.


\section{Implementation}
\label{sec:implementation}

The following subsections detail the implementation of the system's most relevant features.

\subsection{Mobile BVI-Acessible Soundscape Player Prototype}
\label{ssec:mobile-implementation}

The system's logic is mostly separated into multiple MonoBehaviour services and manager classes, which generally communicate by event propagation rather than relying on synchronous calls, facilitating adjustments to individual components without interfering with the rest of the system. Immutable configurations of the game objects responsible for the player controller, ambient sound sources and spatial sound sources are predefined as prefabs and stored in scriptable objects. Also stored in scriptable objects are the audio clips associated with fixed functionality descriptions and cues, such as clock hour indicators and functionality activation/deactivation messages.

The few \gls{UI} elements making up the application's simple interface - the virtual joysticks and display change button - are placed within Unity Canvas elements, and all used assets are \gls{CC0} licensed for free and legally compliant use. The icons used for the display button and the sound sources were sourced from UXWing\footnote{\url{https://uxwing.com/}}, and the joystick visuals from Kenney's Mobile Control assets pack\footnote{\url{https://kenney.nl/assets/mobile-controls}}.

Essentially acting as the user's head and ears within the auditory environment, the player game object is the sole audio listener in scene and has the Steam Audio Listener component attached to it, required to perceive the effects rendered by the Steam Audio plugin. The player object is visually represented by a forward-facing arrow centered on the screen at all times, implemented with a Cinemachine camera targeting it. Triggered by horizontal tilts of the left joystick, rotation of the player occurs in fixed increments of 30 degrees (equivalent to one hour on a clock) with a small cooldown. For simplified locomotion, movement is activated by vertical input in the left joystick and manifests itself only along that same axis of the player's current orientation, translating to directly forward or backward motion.

The prefabs of both ambient and spatial sound sources have attached audio source components to play their respective sound effects. While ambient sounds are not localized at all, the effects produced by spatial sources are fully directional and binaurally rendered with a Steam Audio Source component. Each spatial sound object also contains a separate Steam Audio localized audio source to play its corresponding verbal description when detected by the directional scanner.

The directional scanner functions by casting a ray in the direction of the right joystick's input and checking for a collision with a spatial sound source, triggering a localized narration from a detected source and blocking the sound effects from all other spatial elements while the narration is ongoing. Detections are also visually indicated in the scanner's cone, whose color becomes more intense through increased opaqueness. Listing~\ref{lst:scanner} presents the method in charge of this functionality.
\begin{lstlisting}[style=csharp, caption={Main method for implementing the scanner functionality in C\#.}, label={lst:scanner}]
private void HandleScanner(Vector2 scannerInput)
{
	// Determine the direction in which to scan, relative to the player's orientation
	Vector2 scanDirection = scannerInput.x * transform.right + scannerInput.y * transform.up;
	// Rotate visual to match scan direction
	float angle = Mathf.Atan2(scanDirection.y, scanDirection.x) * Mathf.Rad2Deg;
	// Visual sprite is pointing to Y+, Atan gives angle from X+ to the vector
	// Sprite is rotated 90 degrees from the right, compensate by rotating back 90 degrees
	scannerVisual.rotation = Quaternion.Euler(0, 0, angle - 90f);
	
	// Cast a ray in the scanned direction
	RaycastHit2D raycastHit2D = Physics2D.Raycast(transform.position, scanDirection, _raycastDistance, layerMask);
	if (raycastHit2D.collider is not null)
	{
		// Hit something
		if (_lastCollider is null || _lastCollider != raycastHit2D.collider)
		{
			// Check if the collider is a SpatialSoundSource
			if (raycastHit2D.collider.TryGetComponent(out SpatialSoundSource spatialSoundSource))
			{
				// Set lastCollider to the new collider
				_lastCollider = raycastHit2D.collider;
				// Set lastSpatialSoundSource to the new spatial sound source
				_lastSpatialSoundSource = spatialSoundSource;
				// Trigger the source's narration
				spatialSoundSource.StartNarration();
			}
		}
	}
	else
	{
		// Set lastCollider to null if we are not hitting anything
		_lastCollider = null;
	}
}
\end{lstlisting}

When activated and if there are active spatial sound sources in range, the proximity sensor monitors in real time the closest spatial element within a forward-facing cone with a default amplitude of 90 degrees. The sensor estimates the frequency at which source-localized beeps are emitted according to the distance between the player and the detected source. The lesser this distance, the higher the beeping frequency - indicating frontal proximity to a point of interest, as intended. Listing~\ref{lst:sensor} presents the implementation of the two most relevant methods handling this feature.
\begin{lstlisting}[style=csharp, caption={The two main methods for implementing the proximity sensor in C\#.}, label={lst:sensor}]
private void HandleProximitySensor()
{
	// Find the nearest sound source within the cone
	SetCurrentTargetWithinCone();
	// Only proceed if there is a current target
	if (_currentTarget is null) return;
	// Determine the beep interval based on distance
	DetermineBeepInterval();
	// Handle the beep timer
	HandleBeepTimer();
}

private void SetCurrentTargetWithinCone()
{
	// Get the closest active spatial sound source within the forward cone 
	SpatialSoundSource nearestInCone = SoundSourceManager.Instance.GetSpatialSoundSourcesSortedByDistance(transform.position)
		.Where(source => 
		{
			// Check if the source is not null and is currently playing
			if (source == null || !source.IsPlaying()) return false;
			// Check if the source is within the maximum distance (this transform shares the player's position)
			float distance = Vector3.Distance(transform.position, source.transform.position);
			if (distance > _farThreshold) return false;
			// Calculate the angle between the player's forward direction and the direction to the sound source
			Vector3 directionToSource = source.transform.position - transform.position;
			float angle = Vector3.Angle(transform.up, directionToSource);
			// Check if the angle is within the forward half-angle
			return angle <= _forwardHalfAngle;
		})
		.FirstOrDefault();

	if (nearestInCone is not null)
	{
		// If a target is found within the cone, set it as the current target
		_currentTarget = nearestInCone.transform;
	}
	else
	{
		// If no target is found within the cone, set the current target to null and reset the timer
		_currentTarget = null;
		_timer = 0f;
	}
}
\end{lstlisting}

Touch gesture detection was explicitly programmed from the ground up using the EnhancedTouch library (from Unity's Input System package) which provided essential data such as the number of fingers touching the screen, their screen position and even phase of touch, among other attributes. Such information facilitated a proper implementation of the single and multi-fingered motions available in the application. The same Input System Package was utilized to easily support gamepad controls.

Using the aforementioned HapticFeedback library (in section~\ref{sec:technological-stack}), different intensities of haptic feedback were incorporated into virtual joystick touch and release actions as well as when colliding with the boundaries of the painting, defined by an edge collider.


\subsection{Desktop Beginner-Friendly Soundscape Editor}
\label{ssec:desktop-implementation}

The desktop editor mostly shares the mobile application's separation of responsibilities into manager classes and MonoBehaviour services, also inheriting its primarily event-driven design to avoid unnecessary interdependencies between components. As in the mobile solution, prefabs for the player, ambient and spatial sound sources are stored in scriptable objects. However, unlike the mobile version, these prefab elements are merely visual in development mode. Information regarding a placed element is mapped to the grid cell it resides in, rather than being tied to the game object itself, making it so that the grid is fully responsible for storing the content of its elements and providing it to other parts of the system.

The editor's \gls{UI} was developed in its entirety with Unity's \gls{UI} Toolkit, from the buttons to the popup windows, including the various \gls{UI} elements housed within them. Exceptionally, the background texture consists of a sprite in a Canvas game object so that it remains behind all other \gls{UI} elements built with \gls{UI} Toolkit. Assets are sourced from \gls{CC0}-licensed repositories: UXWing for icons and free-images.com\footnote{\url{https://free-images.com/}} for the background texture image.

In development mode, the editor capitalizes on Unity's built-in grid component as the structural foundation for its grid-based placement system. Its implementation draws heavily from an online grid system video course\footnote{\url{https://www.youtube.com/playlist?list=PLcRSafycjWFepsLiAHxxi8D_5GGvu6arf}}, adapting from this tutorial a custom shader for visualization of the grid and also the application of the State pattern to handle the grid's behavior in different modes: placement, edit and removal. Employing this design pattern, the logic for each mode is fully encapsulated in a dedicated class exposing its own implementation of required state actions. Visualizable in figure~\ref{fig:desktop-editor-state-pattern}, the placement system holds only a reference to a general state object, whose internal state changes and delegates responsibilities accordingly, triggering the appropriate actions.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{state-pattern}
  \caption{State pattern incorporation within the application.}
  \label{fig:desktop-editor-state-pattern}
\end{figure}

Since the very purpose of the editor's Interactive mode is to emulate the soundscape as it would be presented in the mobile application, it repurposes essentially all of the mobile player's core logic, sharing nearly all of its assets, including media resources, prefabs and scripts. Where the editor's Interactive mode differs from the mobile version is in input control, as it uses toggleable buttons for interacting with the auditory environment instead of touch gestures or gamepad controls. It also does not feature haptic feedback, since it does not support a touch interface.

To transfer data between Development mode and Interactive mode, implemented as separate Unity scenes, scene information of the devised environment in Development mode is locally persisted as a JSON file in Unity's persistent data path. The structure of the persisted information is defined in the \textit{SceneData} class, which is exposed in figure~\ref{fig:desktop-editor-scene-data}. When Interactive mode is enabled, this file is parsed and translated into a fully functional preview of the soundscape. So that the audio files, as the heaviest media resources to be loaded into Interactive mode, do not bottleneck the application and significantly delay loading the remaining scene information, these are cached in parallel into a dictionary. Listing~\ref{lst:scene-data-loader} exposes the implementation of the method which loads and sets the information pertaining to a devised scene into Interactive mode, as well as the ancillary function for asynchronously caching the existing audio files.As mentioned in section~\ref{sec:system-limitations}, the current editor prototype is explicitly programmed to delete the file upon closing, effectively acting as a one-shot creative environment that leaves no residual files. This decision was made in light of the possibility that some user study participants would undertake the system evaluation remotely, using the application on their desktops.
\begin{lstlisting}[style=csharp, caption={Methods for loading scene information into Interactive mode and asynchronous audio caching in C\#.}, label={lst:scene-data-loader}]
public async Task LoadSceneData()
{
	// Load the generated scene's data
	SceneData sceneData = SaveSystem.LoadSceneData();
	// Set cell size based on scene data
	_cellSize = sceneData.cellSize;
	// Set background image
	BackgroundImageController.Instance.SetBackgroundImage(sceneData.pathToBackgroundImage);
	// Spawn player first, to give time to load audio listener and camera controller
	await PlaceWorldPlayerObject(sceneData);
	// Load all audio files asynchronously and await for completion
	await LoadAllAudioFiles(sceneData);
	// Set global narration
	LoadGlobalNarration(sceneData);
	// Place ambient and spatial sound objects
	PlaceWorldSoundObjects(sceneData);
}
	
private async Task LoadAllAudioFiles(SceneData sceneData)
{
	// Get all unique audio file paths from the scene data
	HashSet<string> audioFilePaths = GetAllAudioFilePaths(sceneData);
	// Use a dictionary to map file paths to loading tasks
	Dictionary<string, Task<AudioClip>> loadingTasks = audioFilePaths
		.ToDictionary(path => path, AudioLoaderAsync.LoadClip);
	// Await all loading tasks to complete in parallel
	await Task.WhenAll(loadingTasks.Values);
	// Populate the audio clips cache with the loaded clips
	foreach (var kvp in loadingTasks)
	{
		AudioClip clip = await kvp.Value;
		_audioClipsCache[kvp.Key] = clip;
	}
}
\end{lstlisting}
\label{lst:scene-data-loader}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{scene-data}
  \caption{\textit{SceneData} class holds the soundscape information passed between the application's two modes: Development and Interactive.}
  \label{fig:desktop-editor-scene-data}
\end{figure}

As outlined in section~\ref{sec:technological-stack}, file system interactions in the editor, such as selecting the painting or audio files from a local desktop directory via a file dialog, were incorporated into the application using the UnityStandaloneFileBrowser.
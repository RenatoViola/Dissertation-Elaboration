\typeout{NT FILE chapter4.tex}%

\chapter{Proposed Solution}
\label{cha:proposed-solution}

\epigraph{
  This chapter provides a detailed overview of the proposed system, introducing the BVI-inclusive mobile soundscape player and the soundscape editor for Windows. It also addresses the technological stack chosen for the solution's implementation, followed by a discussion on the implementation of both prototypes composing the system. An exploration of the initial research and refinement of the proposed concept concludes this chapter.
}{}

\section{Proposal Overview}
\label{sec:proposal-overview}

The solution proposed in this dissertation is a bipartite system intended to, above all, facilitate BVI-inclusive interpretation of visual paintings, via immersive and highly interactive soundscapes based on museum paintings. While clearly distinct in purpose and also in the audience they serve, these two allied parts provide both soundscape interaction and intuitive creation. Figure~\ref{fig:concept-img} attempts to illustrate the intended approach from a purely conceptual standpoint.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{concept-cropped}
  \caption{Concept representation. Background image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado}).}
  \label{fig:concept-img}
\end{figure}

The first and ultimately core component is a mobile application, which serves as an interactive and accessible soundscape player for BVI visitors. Through its available SATs, the application serves as a vehicle for users to autonomously navigate the artwork and learn about it or even feel it, via various sound effects (be it ambience or localized sound), verbal descriptions and customizable sound settings, among other considerations.

The second component is a desktop soundscape editor that enables intuitive creation of these auditory environments, from design to testing, and eventually refinement. This tool targets curators whom are not required to have any prior experience in sound design or programming, abstracting the complexities of spatializing sound sources and optimizing for accessibility, by way of a straightforward interface. While the editor plays a essential role in the system, it fundamentally serves as a supplement to the mobile application - as the means to produce the content available within it.

The following two sections detail these components further, starting by the mobile constituent (section~\ref{ssec:mobile-soundscape-player-overview}) and concluding with the desktop editor (section~\ref{ssec:soundscape-editor-overview}).

\subsection{Mobile Soundscape Player}
\label{ssec:mobile-soundscape-player-overview}

The mobile application's interface (portrayed in figure~\ref{fig:mobile-overview-interface}) displays a top-down \gls{2D} representation of a soundscape, with large and high-contrasting elements representing directional sound sources, making them easier to distinguish within the environment. This approach makes use of standard \gls{BVI} accessibility practices for visual interfaces in other research~\cite{ahmetovic2021musa,drossos2015accessible,simao2018jogo} and having spatial sounds assigned to points of interest in the painting was inspired by Yang et al.'s~\cite{yang2019audio} study. Ambient sound sources are also present but permanently hidden to prevent users with partial sight from misinterpreting them for localized sounds. The user’s head and ears are represented in the scene by a small arrow that always faces upwards on the screen, regardless of the user’s orientation within the virtual space.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{placeholder}
  \caption{The three available display modes of the mobile prototype, in the order of: Immersive, Accessible and Minimalist.}
  \label{fig:mobile-overview-interface}
\end{figure}

Still concerning the visual interface, the application offers three different display modes for users with residual vision (refer to figure~\ref{fig:mobile-overview-interface}), and since these are purely visual, alternating between them is executed via a visual button in the upper left corner of the screen:
\begin{itemize}
  \item \textbf{Immersive mode (default)} Displays the background painting and the sound sources.
  \item \textbf{Accessible mode:} A pure white background with further contrasting sound sources, where virtual joysticks are darker for better contrast.
  \item \textbf{Minimalist mode:} Displays the background painting but hides the sound sources, with the virtual joysticks made more transparent for minimal distraction.
\end{itemize}

So that users can properly detect the directionality and proximity of sound sources through binaural cues, the use of headphones is required for an accurate perception of spatialized sounds.

Navigation through the virtual space is enabled via a joystick control scheme, inspired by Nair et al.’s~\cite{nair2022uncovering} Dungeon Escape game and other classic \gls{3D} games such as the early Resident Evil\footnote{https://game.capcom.com/residentevil/en/} titles. The application supports both gamepad and touch controls, with these two schemes resembling each other as much as possible. For example, most directional buttons on the gamepad trigger the same actions as swipe gestures on the touch interface. When a gamepad controller is connected to the smartphone, the virtual joysticks are automatically hidden and disabled, returning to active if the controller is disconnected.

The left joystick controls the user’s movement within the painting: vertical tilts move the user forward or backward, and horizontal tilts execute snap rotations in fixed increments of 30 degrees, complemented by distinct audio cues. If the user reaches the border of the painting, movement is halted in the direction of the boundary. In addition, all sounds are muted and the smartphone emits a strong vibration. Upon returning to the in-bounds area by moving away from the boundary, the sounds resume and so does normal movement.

The right joystick acts as a directional scanner akin to to Nair et al.'s~\cite{nair2021navstick} NavStick, though in \gls{2D} rather than in \gls{3D}. By tilting the joystick in a particular direction, the user can hear a localized narration of the first object in their line of sight, through a predefined speech output. Figure~\ref{fig:mobile-overview-joystick} exemplifies this scanner in practical use. To assist with a more literal interpretation of the environment, a global verbal description of the entire scene can be triggered, during which all spatial sounds pause and then resume once the narration finishes. The idea of using blocking sounds (narrations) and interruptible sounds (spatial sounds in this particular case) was inspired by Drossos et al.'s~\cite{drossos2015accessible} implementation of a BVI-accessible Tic-Tac-Toe game.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{placeholder}
  \caption{Directional scanner in use: on the left the scanner has not detected an element while on the right it has detected a spatial sound source and narrates it.}
  \label{fig:mobile-overview-joystick}
\end{figure}

An additional, no less important \gls{SAT} is available: a proximity detection tool, inspired by the rear sensors in modern vehicles, which signal for nearby collidable obstacles. Much in the same manner, this toggleable proximity sensor emits an intermittent sound that increases in frequency as the user approaches an object (a spatial sound source in this context). Contrasting that of the rear of a car, the implemented sensor uses a forward cone of detection, identifying the closest spatial sound source within range. There are three range levels: far, intermediate, and close. Each corresponds to progressively more frequent and louder localized beeps, as the user approaches the detected sound source.

To regulate sensory and cognitive overload, a significant concern for \gls{BVI} individuals~\cite{guerreiro2023design,nair2022uncovering}, users may control the density of sensory inputs to a degree. One such example is the ability to toggle ambient sound on or off. Additionally, influenced by Guerreiro et al.'s~\cite{guerreiro2023design} framework to support BVI-inclusive auditory representations, users can control the number of active spatial sound sources at any given moment. To further mitigate cognitive overload and since Guerreiro et al.'s research stated that fully concurrent auditory feedback could be mentally overwhelming for BVI individuals, the application offers two spatial sound reproduction modes: simultaneous and sequential. As the name implies, in simultaneous mode, all spatial sounds play concurrently. In sequential mode, sounds are consecutively played by order of proximity, lasting for a fixed amount of time. Once they have all played, the reproduction cycle starts over, with the order updated based on the user's position at the time.


\subsection{Soundscape Editor for Windows}
\label{ssec:soundscape-editor-overview}

Drawing inspiration from Ferreira’s~\cite{ferreira2021creating} Immerscape, this editor is to serve as an intuitive soundscape creation tool for curators without prior expertise in sound design or programming, enabling them to translate visual paintings into the BVI-inclusive auditory environments present in the mobile application discussed in section~\ref{ssec:mobile-soundscape-player-overview}. While it was initially planned for this tool to allow for the exportation of created soundscapes to the mobile application, connecting these two components, the feature was not implemented in the prototype due to time constraints. Section~\ref{sec:future-work} delves into how this feature may be addressed in a future iteration of the system.

The editor is designed to minimize complexity for curators by enabling interaction primarily through the mouse. All file management, such as choosing a painting image and selecting audio files is handled by means of an intuitive file browser. This versatile browser allows users to organize and fetch media resources from wherever in the desktop they see fit.

Initially, the application was meant to be much more similar to Immerscape than it currently is, featuring a 3D development environment with predefined acoustic properties like reverberation and resonance, alongside a block-based placement system similar to Minecraft\footnote{https://www.minecraft.net/en-us}’s creative mode. However, during the mockup elaboration stage, it was concluded that in the context of BVI accessibility, introducing additional complexity to the scene in the form of acoustic properties, like reverberation and resonance, could bring about uneccessary complexity and cognitive overload. Most importantly, enforcing a 3D development environment could be cumbersome and time-consuming for curators, who would be required to interpret and translate a 2D painting into a 3D auditory environment faithfully embodying the artwork. As a result, the decision was made to simplify the development system and have it be in 2D with a grid-based placement system in place, reducing cognitive strain while retaining the two-dimensional essence of the original painting.

The workflow within the application is divided into two main modes: Development mode and Interactive mode. As a quick introduction, it is in Development mode that curators may design the soundscape, while Interactive Mode provides a built-in interactive preview of the devised soundscape, simulating how it would be experienced by the end user in the mobile application.

In Development mode (exposed in figure~\ref{fig:desktop-editor-development-mode}), users begin by selecting a painting (via the file browser), which is displayed as a background image in the scene. Zoom level is automatically adjusted to ensure the image appropriately fits the screen, and the user may further zoom in or out to taste. Setting a global narration to represent the artwork is executed identically to defining the background image. Placement, editing and removal of elements in the scene is done via a simple grid-based building system reminiscent of city builder games, but in 2D. This rectangular grid layout dinamically fits to the painting's dimensions and is customizable in both the size of its cells and the thickness of its lines, overlaying the painting when enabled.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{placeholder}
  \caption{Development mode. Background image is \textit{O Bosque Sagrado} by Maria Benamor, 1975. Collection of Museu Nacional Gr\~ao Vasco. (Adapted from~\cite{o-bosque-sagrado})}
  \label{fig:desktop-editor-development-mode}
\end{figure}
With the painting set, curators may place elements in the grid, of which there are three clearly distinct types, selectable from the panel at the bottom center of the screen:
\begin{itemize}
  \item \textbf{Ambient Sound:} Represents general background noise and ambience whose effect is independent of spatial placement within the grid. Its editable attributes are: \textit{name, audio file \& volume, looping, fade-in and fade-out};
  \item \textbf{Spatial Sound:} Represents a fully localized sound effect, playing only from the specific position in which it was placed. Attributes include: \textit{all attributes from ambient sound, narration audio file \& correspondent volume and volume rolloff settings, to simulate how sound intensity decreases with distance};
  \item \textbf{Player:} Represents the person exploring the scene and acts as the controller for movement and hearing within it. This element's position in the grid determines the end user's initial position. Its customizable attributes are: \textit{initial clockhour orientation, movement speed and proximity sensor settings, in which the amplitude of the detection cone is included}.
\end{itemize}

The grid remains visible only when adjusting its properties or when active for placing, editing, or removing elements and can be closed via a button in the lower left corner, which only appears when the grid is enabled. Placing an element consists only on having it actively selected and clicking on an empty grid cell to place it, and no cell can have more than one element assigned to it. Editing and removing elements work function identically except that a cell must already have an element to be valid to act upon. When editing an element (see figure~\ref{fig:desktop-editor-editing-popups}), changes are persisted automatically rather than via an explicit submit button. Additionally, the radiuses of volume rollof in spatial sound elements and proximity definition in the player element's sensor are visualizable in the grid layout via overlapping circles of decreasing opacity.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{placeholder}
  \caption{Editing popups for a spatial sound element and the player element. The rightmost image represents the reach of the proximity sensor.}
  \label{fig:desktop-editor-editing-popups}
\end{figure}

When users want to preview the soundscape they devised as it will be made available to the visitor audience, they should use headphones for accurate spatial sound perception and enter Interactive mode. This mode effectively emulates the mobile application covered in section~\ref{ssec:mobile-soundscape-player-overview}: they may move around and scan the scene with the virtual joysticks in the lower corners of the screen and use every functionality available in the mobile application, but trigger them through buttons rather than touch gestures. At any time, users may return to Development mode, easily alternating between the two.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{placeholder}
  \caption{Interactive mode.}
  \label{fig:desktop-editor-interactive-mode}
\end{figure}


\section{Technological Stack}

The Unity\footnote{https://unity.com/} game engine, namely Unity 6, was selected as the primary development environment for the desktop soundscape editor and mobile application prototype. The main factor that solidified this choice was its adoption among some of the most relevant related work~\cite{ferreira2021creating,guerreiro2023design,nair2021navstick,nair2022uncovering,simao2018jogo,yang2019audio} analyzed in chapter~\ref{cha:related-work}.

While Unity is mainly known for its role in game development, its versatility has led to widespread use in all sorts of interactive \gls{2D} and \gls{3D} experiences across various industries~\cite{techical-overview-unity,what-is-unity}. Most importantly, it provides extensive cross-platform build support, including the hardware this dissertation concerns itself with: Windows (for the editor application) and Android (for the mobile interaction).

Aside from its powerful rendering capabilities, Unity has a friendlier learning curve than other mainstream \gls{3D} game engines like Unreal Engine\footnote{https://www.unrealengine.com/} and sports a convenient package manager extending its functionality with third-party libraries.

Unity's built-in audio system already provides sturdy audio management, featuring distance attenuation, spatial blend, and adjustable volume rolloff, among other settings. However, while this system suffices for rudimentary \gls{3D} audio, it is limited for the purposes of this work, lacking binaural rendering and other relevant capabilities. There are however specialized plugins for advanced spatial audio that address the vanilla system’s limitations by adding some of the essential and advanced spatialization functionalities it lacks.

The Steam Audio\footnote{https://valvesoftware.github.io/steam-audio/} framework was selected to address this base engine limitation and has been actively maintained, well-documented, and compatible with Unity engine updates. Furthermore, this toolkit was also used in NavStick~\cite{nair2021navstick} and Dungeon Escape~\cite{nair2022uncovering}, two of the most influential works for this proposal, motivating its adoption. Steam Audio supports high-fidelity HRTF-based binaural rendering and geometry-based occlusion, reflection, and reverberation effects, among other spatial features contributing to natural sounding immersion. Though out of these features, only binaural rendering was employed (with the built-in Steam Audio \gls{HRTF}) and other properties retained their default values \textbf{(TODO: Revisitar propriedades do Steam Audio quando tiver o meu PC)}. While the specific library to which its default \gls{HRTF} dataset belongs is not explicitly stated in official documentation, some online discussions suggest it may be based on Phonon\footnote{https://www.impulsonic.com/what-is-trueaudio-phonon/}, however this remains an assumption.

Aside from Steam Audio, several external packages were integrated within the Unity environment in order to agilize development. One such example is DOTween\footnote{https://assetstore.unity.com/packages/tools/animation/dotween-hotween-v2-27676}, an animation engine employed in both the mobile and desktop components of the system, for smooth transitions visually and also in sound (namely the fade effects). For haptic feedback on the smartphone, the HapticFeedback\footnote{https://github.com/CandyCoded/HapticFeedback} library was used, supporting vibration patterns of varying intensities. File system interactions in the desktop editor such as opening a file from a local desktop directory via a file dialog were supported by the UnityStandaloneFileBrowser\footnote{https://github.com/gkngkc/UnityStandaloneFileBrowser}, a third-party tool alongside the above mentioned HapticFeedback library. Lastly and also the most essential, the Input System Package\footnote{https://docs.unity3d.com/Packages/com.unity.inputsystem@1.14/manual/index.html} was incorporated to handle all input, from keyboard bindings, to gamepad controls and even the full implementation of the touch interface, effectively replacing the entirety of Unity's built-in input system.

Apart from Unity and the C\# programming language in which its scripts are written, the Python language was used for a single script (outside of the system itself) which interacted with the Google Cloud Text-to-Speech API\footnote{https://console.cloud.google.com/marketplace/product/google/texttospeech.googleapis.com}. The intent of this script was to generate narrations for functionalities such as clock hours for the orientation feature and activation/deactivation sound effects. Conveniently, it was also used to generate the consistent verbal descriptions utilized in the prototypes presented to end users.


\section{Implementation}
\label{sec:implementation}

\subsection{Mobile BVI-Acessible Soundscape Player Prototype}
\label{ssec:mobile-implementation}

\lipsum[1-5]

\subsection{Desktop Beginner-Friendly Soundscape Editor}
\label{ssec:desktop-implementation}

\lipsum[1-5]

\section{Initial Research \& Concept Refinement}
\label{sec:initial-research}

Prior to the system's technical implementation, conceptual designs of the mobile soundscape player and the desktop editor were produced in Figma, illustrating the key functionalities envisioned for each application. At this stage, the mobile application was identified as the main component to prove the concept, since it would directly serve BVI users.

The first implementation efforts therefore focused on an early version of the mobile prototype, already including most of the features described in section~\ref{ssec:mobile-soundscape-player-overview}. This build was sufficient to demonstrate the feasibility of the proposed concept for accessible and interactive interpretation of a painting, as was ultimately intended. However, compared to the final application it lacked ambient sound regulation, the proximity sensor, and haptic feedback, among several other adjustments - as these hadn't yet been planned at the time. Along with the mockups of the desktop editor, some of the mobile functionalities were presented in a remote meeting with researchers from the Department of Conservation and Restoration of NOVA School of Science and Technology. The researchers saw potential in the idea and expressed interest in its evolution.

The most important stage of this early phase came in late May, when the mobile prototype was tested on-site at the Raquel e Martin Sain Foundation\footnote{https://www.fundacao-sain.pt/} - a social solidarity institution dedicated to the professional education and social integration of BVI individuals. Two blind participants volunteered their time: Cláudia Pires, a student at the foundation, and Ana Inês Colares, one of its instructors. Both of these interviews followed a simplified protocol with similarities to that of the final study but with less specific tasks and more open-ended questions, for the goal was refinement rather than evaluation. The painting "O Bosque Sagrado"~\cite{o-bosque-sagrado} by Maria Benamor was used as the painting for the test scenario, also being used in the final study of the application (section~\ref{sec:mobile-sonic-painting-exploration}).

The first interviewee, Cláudia Pires, pointed to several areas for improvement in the experience. She felt that ambient sound should be lower and that the sound effects used for movement and rotation were too similar, sometimes hard to distinguish. Though she could discern sound direction clearly, perceiving proximity to the sound sources was more difficult. While she found the directional scanner useful, she noted that it required alignment that was much too precise, with narrations cutting off if not perfectly targeted. Cláudia also reported little difference between sequential and simultaneous reproduction modes and sometimes had difficulty tracking how many sounds were active. The clock hour orientation feature was very helpful to her but she stated that it should have been introduced earlier in the protocol. Finally, she strongly preferred the console scheme over the touch gestures, suggesting haptic feedback and larger virtual joysticks as possible improvements to touch usability.

Many of the points brought up by Cláudia were echoed by the second interviewee, Ana Inês Colares, who also provided further insights. Accentuating the importance of the clock hour orientation system, she too felt that this feature should be introduced in an earlier task. Ana Inês suggested that sound sources should increase in volume more noticeably when approached and that ambient sound should be toggleable, as it could distract from the other elements at times. Like Cláudia, she noticed little difference between the two reproduction modes and found sound source direction easier to perceive than proximity, occasionally struggling to locate specific sounds in the environment. She also preferred the console controls and noted the difficulty of touch gestures and especially virtual joysticks without haptic feedback. Lastly, she proposed the addition of a menu to allow switching between different soundscapes, if the application were to support several painting representations. Most importantly, from the conversation with Ana Inês and her emphasis on the amplification of sound according to proximity, derived the idea for the currently implemented proximity sensor.

The purpose of this early study was to garner real input from the individuals to whom the application was intended, and in this way assess whether the proposed features were indeed adequate for BVI needs, rather than only in theory. These preliminary interviews were fundamental in shaping the application into what it eventually became, as both participants provided detailed feedback which revealed not only the prototype's shortcomings but also some of its strengths. As much as possible, their suggestions were incorporated into the application's final design and in the later evaluation (section~\ref{ssec:mobile-results-overview}) these proved both practical and meaningful.
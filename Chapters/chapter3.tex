%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{Related Work}
\label{cha:related-work}

% epigraph configuration
\epigraphfontsize{\small\itshape}
\setlength\epigraphwidth{12.5cm}
\setlength\epigraphrule{0pt}

\epigraph{
  This chapter reviews relevant research and applications related to the topic, including technologies and approaches that have been used to address accessibility for visually impaired individuals. It identifies existing gaps and highlights the unique aspects of the proposed solution in comparison to current state-of-the-art techniques, as well as some similarities.
}{}

\section{Immersive Audio Experiences in Cultural and Learning Environments}

Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero venenatis faucibus. Nullam quis ante. Etiam sit amet orci eget eros faucibus tincidunt. Duis leo. Sed fringilla mauris sit amet nibh. Donec sodales sagittis magna. Sed consequat, leo eget bibendum sodales, augue velit cursus nunc,

\subsection{Eyes-Free Art}

Looking to enhance the accessibility of visual art to BVI individuals and go beyond the shortcomings of the typical audio descriptions or guides, Rector et al.~\cite{rector2017eyes} designed “Eyes-Free Art”, a novel approach to sonically interacting with 2D art, aiming to be both aesthetically stimulating and engaging. 

It is in essence, a carefully crafted proxemic audio interface that, mirroring the conventional intuition of visual proxemic interfaces, renders more detail as proximity to the piece increases. To define such proximity, a Microsoft Kinect device was used to track the user’s position and movements, not only to determine the user’s distance to the painting but also if it is facing towards it. The audio interpretation a user hears varies according to the proxemic zone in which it finds itself, of which there are four distinct and equally sized ones (Figure~\ref{fig:eyes-free-art-img}). Upon entering any zone, the user is verbally alerted to where it is and may continue moving between zones, spending as much time as it wants in each.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{eyes-free-art}
  \caption{The four zones of the Eyes-Free Art proxemical interface (Adapted from~\cite{rector2017eyes}).}
  \label{fig:eyes-free-art-img}
\end{figure}

The furthest zone consists only of background music used to set the mood of the piece. It is followed by a sonification area aimed at communicating the painting’s chromatic diversity through musical features. The second closest zone, sound effects, highlights the painting’s literal aspects such as the type of objects and their spatial correlation. The final and most detailed of zones consists of a manually curated verbal description. 

Some initial interviews were conducted, from which Rector et al. noted the importance of using commodity technology (promoting control and independence) and including both the literal and subjective aspects of a painting.

A final evaluation with 13 BVI participants attested to the success of this implementation, as patrons felt immersed and had a rich experience interpreting the art work.

Eyes-Free Art resonates with the current dissertation in several ways. Most importantly, in its integration of zones of differing detail according to proximity, allowing users to explore at their own pace and at their desired level of detail, closely aligning with our goal of promoting independence and interactivity. Furthermore, it also addresses the mapping of visual elements to sound, though the current work focuses on spatial audio rather than sonification.


\subsection{Audio-augmented museum experiences with gaze tracking}

Aiming to enrich the perception of landscape and genre paintings, Yang et al.~\cite{yang2019audio} track a visitor’s gaze and spatialize sounds for drawn objects and scenes within the paintings. Personalizing the audio output based on the user’s gaze, the system amplifies the sounds directed at the viewers focal point, attenuating the rest. 

Gaze and pose tracking required an eye-tracker and a connected laptop in a backpack. Additionally, headphones were used for spatial audio playback (Figure~\ref{fig:gaze-img}). The propagation of sound was dynamically simulated according to a user’s gaze and pose via the Google Resonance Audio SDK, while the Unity3D game engine was employed to model the room and map the various sound sources.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{gaze-1}
  \caption{Two of the paintings used in the application. Blue audio icons represent virtual sounds accurately spatialized relative to the user (b). (Adapted from~\cite{yang2019audio}).}
  \label{fig:gaze-img}
\end{figure}

A user study with 14 young adults revealed some challenges regarding the consistency of the eye-tracking and differences in preference across individuals, such as the amplification of sound and the smoothness of its adjustment. The experience was still positively received overall, as it helped most users focus on areas of interest, some even feeling guided by their gaze.

While gaze tracking proves itself a mostly intuitive approach to dynamic audio spatialization and interactivity with an artwork, it is not the most appropriate technique in the context of our work, which focuses on BVI users.

Nonetheless, this study provides valuable insights to our own. Akin to our proposal’s proximity-based 3D audio, it narrates a painting’s visual elements by spatially embedding sounds to specific points of interest and dynamically adjusting their intensities. Furthermore, there is relevance in learning from the challenges highlighted by Yang et al., namely in ensuring smooth audio transitions, responsiveness and accomodating for some degree of personal preference, through personalization.


\section{Accessibility in Auditory Representations of Art and Virtual Environments}

Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero venenatis faucibus. Nullam quis ante. Etiam sit amet orci eget eros faucibus tincidunt. Duis leo. Sed fringilla mauris sit amet nibh. Donec sodales sagittis magna. Sed consequat, leo eget bibendum sodales, augue velit cursus nunc,

\subsection{Navmol}

Navmol~\cite{fartaria2013navmol} is a molecular browser and editor specifically designed for blind and visually impaired users, aiming to provide BVI accessibility in the higher education of chemistry. 

Via a speech synthesizer, it provides an auditory portrayal of the atomic composition of complex molecular structures. Such configurations are bidimensionally depicted using the analogue clock metaphor, consisting of mapping directions to the positions of a clock and well known among the BVI community. As some users retain some degree of sight, the program has a simple graphical interface (Figure~\ref{fig:navmol-img}) with them in mind, visually displaying the selected atoms and some molecular contours.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{navmol}
  \caption{Navmol's clock system, where carbon-3 has neighbors at 6 o'clock and 10 o'clock, respectively carbon-4 and carbon-2 (Adapted from~\cite{fartaria2013navmol}).}
  \label{fig:navmol-img}
\end{figure}

Rodrigues~\cite{rodrigues2015navmol} merged the simple clock analogy with the application of HRTFs on the auditory signal generated by the existing Navmol program at the time (version 2.0), in order to create realistic directional sound cues, perceived to derive from where the atom is positioned. Usability tests demonstrated the efficacy of this integration, with users achieving an average task accuracy of 95.7\% in identifying and navigating molecular structures.

Knowing that HRTF performance may greatly vary across users due to inter-individual differences in morphology, Rodrigues conducted a study on the performance of 53 distinct HRTF measurements. It confirmed a significant variation in performance across different HRTF datasets for individual users. The five most consistently well performing measurements – KEMAR, CIAIR, IRC05, IRC25 and IRC44, were then selected for use in one additional study, motivated by the significant variation in performance across different HRTF datasets for individual users. The necessity to allow users to select their preferred HRTF dataset was made apparent and Navmol was updated accordingly.

Rodrigues’ work very much aligns with the goals of this dissertation, as it highlights not only the efficacy of spatial audio (integrated through HRTFs) in the perception of spatial and structural information but also addresses the suitability (or lack thereof) of specific HRTFs to distinct users, allowing them to tailor their own experience to a degree.


\subsection{MusA}

Ahmetovic et al.~\cite{ahmetovic2021musa} proposed MusA intending to address the limitations of traditional artwork accessibility methods, such as audio guides. MusA is a mobile application that leverages AR to provide interactive and accessible descriptions of paintings to low vision visitors. These descriptions are structured into chapters, each representing a specific area of the artwork, and are linked to an image overlayed on the artwork with a contour highlighting the described section.

The application features artwork recognition via the mobile’s camera, interactive navigation across chapters, and touch-based overlays. It was specifically designed for users with some residual sight, presenting a clutter-free and to the point interface (Figure~\ref{fig:musa-img}) compatible with system magnifiers, enlarged fonts and adjustable contrast filters.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{musa}
  \caption{Some screens of the MusA app. The first two screens respectively correspond to chapter navigation and selection in its first iteration. The third screen corresponds to the second iteration's virtual mode (Adapted from~\cite{ahmetovic2021musa}).}
  \label{fig:musa-img}
\end{figure}

After an initial user testing with LV participants identifying some challenges, a second and final iteration of the app incorporated audio and haptic feedback, higher contrast contours, and a zoom-supportant virtual mode designed to replace AR in case the user can’t frame the painting continuously. 

User studies revealed MusA to be a significantly more engaging and user-friendly experience than a traditional audio guide, promoting freedom in exploration. Despite some issues in overlay clarity, MusA was effective in supporting the needs of low-vision people and they were pleased by the ability to use the app at home and in their own device.

The work of Ahmetovic et al. parallels that of this dissertation in certain aspects, as alike the chapter navigation available in MusA, the proposed interactive soundscape will offer users control over what details they wish to explore and focus on specific features of the artwork, only with proximity-based or gyroscopic interactions. Their findings show that even for low vision users, visual feedback through properly contrasting overlays enriches the perception of a painting’s structure and details. As such, we mustn’t undervalue visual cues and should ensure low visual clutter alongside proper visibility settings, such as contrast and font size.



\section{Leisurely and BVI Inclusive Applications}

Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero venenatis faucibus. Nullam quis ante. Etiam sit amet orci eget eros faucibus tincidunt. Duis leo. Sed fringilla mauris sit amet nibh. Donec sodales sagittis magna. Sed consequat, leo eget bibendum sodales, augue velit cursus nunc,

\subsection{LEAP Tic-Tac-Toe}

Drossos et al.~\cite{drossos2015accessible} adapted an audio-only version of Tic-Tac-Toe to be made both appealing and accessible to visually impaired children, by empowering it with sonic displays.

Simple as it is, Tic-Tac-Toe is a visually reliant game and spatial perception alongside game state, positioning and rules all posed a significant challenge when designing it to be BVI accessible. Additionally, people with residual vision must also be accounted for, enabling them to complement their auditory experience with their remaining vision.

To this end, they designed and implemented not only the game itself but also a custom game engine and audio engine specifically intended at the effective development of audio games accessible to the visually impaired. Within the game world represented by the game engine, static objects are the most common type of game object, in which sound objects are included.

Carefully ensuring that BVI auditory needs were met, Drossos et al. categorized sound objects into three types that when collectively employed, enable the development of any stage, be it simple or complex. These are classified as: Standard (no interaction with the game, ambient soundtrack is an example), Interruptible (sounds that provide little more than redundant or aesthetic information) and Blocking (sounds carrying important information requiring special attention, interruptible sounds are discarded).

The audio engine functions as the API serving the audio to these objects and handles the sound settings as well as the sonic display implementation. Auditory icons (recognizable real-world sounds) and earcons (synthesic sounds) are used for game state awareness while binaural processing (utilizing the KEMAR HRTF library) is used for conveying spatial details.

The game’s GUI is simple and employs intense contrast alongside large optical elements (Figure~\ref{fig:tic-tac-toe-img}), accomodating for users with residual sight. The auditory interface provides constant feedback of the effects for every action and players sense direction and three-dimensionality through the localized sound cues. In addition, there are pre-recorded audio instructions for each game component.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{tic-tac-toe}
  \caption{Part of the Tic-Tac-Toe game's graphical user interface (Adapted from~\cite{drossos2015accessible}).}
  \label{fig:tic-tac-toe-img}
\end{figure}

In user studies, visually impaired children received the game very positively, though many were experiencing an accessible game for the first time.

Drossos et al.’s findings are of value for this dissertation as they address challenges we will inevitably come across such as the need for constant, concurrent and discernible audio feedback, as well as conveying localization information through the sounds in order for users to navigate themselves around the soundscape. Additionally, they also address relevant limitations as in not all sounds being equally effective for spatial localization and the varying spatial awareness among users.


\subsection{NavStick}

\section{Tools for Soundscape Creation}

Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero venenatis faucibus. Nullam quis ante. Etiam sit amet orci eget eros faucibus tincidunt. Duis leo. Sed fringilla mauris sit amet nibh. Donec sodales sagittis magna. Sed consequat, leo eget bibendum sodales, augue velit cursus nunc,

\subsection{Immerscape}

\subsection{Design Space Considerations}

\section{Concluding Remarks}

Objective: Provide a synthesis of the reviewed work, highlighting gaps and considerations relevant to your proposed solution.

Topics to Include:
Key limitations of existing immersive audio systems and accessibility efforts.
Opportunities for innovation in soundscape design and spatial audio accessibility.
Final considerations for designing a tool that balances accessibility, interactivity, and usability for both creators and BVI users.

Link to Dissertation: Summarize how your proposed solution builds upon and addresses the shortcomings identified in prior research.
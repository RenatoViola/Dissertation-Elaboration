%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{Related Work}
\label{cha:related-work}

% epigraph configuration
\epigraphfontsize{\small\itshape}
\setlength\epigraphwidth{12.5cm}
\setlength\epigraphrule{0pt}

\epigraph{
  This chapter reviews relevant research and applications related to the topic, including technologies and approaches that have been used to address accessibility for visually impaired individuals. It identifies existing gaps and highlights the unique aspects of the proposed solution in comparison to current state-of-the-art techniques, as well as some similarities.
}{}

\section{Immersive Audio Experiences in Cultural and Learning Environments}

Objective: Review how immersive audio technologies have been applied to enhance engagement in cultural and educational settings.

Topics to Include:
Use of spatial audio, binaural audio, and Ambisonics in museums, galleries, and exhibitions.
Examples of audio-enhanced educational content, such as guided tours or virtual museum experiences.
Benefits of immersive audio for storytelling and conveying emotional or spatial dimensions of exhibits.
Case studies demonstrating the impact of these technologies on user engagement.

Link to Dissertation: Highlight the potential of immersive audio as a tool for BVI accessibility and artistic appreciation.

\subsection{Eyes-Free Art}

Looking to enhance the accessibility of visual art to BVI individuals and go beyond the shortcomings of the typical audio descriptions or guides, Rector et al.~\cite{rector2017eyes} designed “Eyes-Free Art”, a novel approach to sonically interacting with 2D art, aiming to be both aesthetically stimulating and engaging. 

It is in essence, a carefully crafted proxemic audio interface that, mirroring the conventional intuition of visual proxemic interfaces, renders more detail as proximity to the piece increases. To define such proximity, a Microsoft Kinect device was used to track the user’s position and movements, not only to determine the user’s distance to the painting but also if it is facing towards it. The audio interpretation a user hears varies according to the proxemic zone in which it finds itself, of which there are four distinct and equally sized ones (Figure~\ref{fig:eyes-free-art-img}). Upon entering any zone, the user is verbally alerted to where it is and may continue moving between zones, spending as much time as it wants in each.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{eyes-free-art}
  \caption{The four zones of the Eyes-Free Art proxemical interface (Adapted from~\cite{rector2017eyes}).}
  \label{fig:eyes-free-art-img}
\end{figure}

The furthest zone consists only of background music used to set the mood of the piece. It is followed by a sonification area aimed at communicating the painting’s chromatic diversity through musical features. The second closest zone, sound effects, highlights the painting’s literal aspects such as the type of objects and their spatial correlation. The final and most detailed of zones consists of a manually curated verbal description. 

Some initial interviews were conducted, from which Rector et al. noted the importance of using commodity technology (promoting control and independence) and including both the literal and subjective aspects of a painting.

A final evaluation with 13 BVI participants attested to the success of this implementation, as patrons felt immersed and had a rich experience interpreting the art work.

Eyes-Free Art resonates with the current dissertation in several ways. Most importantly, in its integration of zones of differing detail according to proximity, allowing users to explore at their own pace and at their desired level of detail, closely aligning with our goal of promoting independence and interactivity. Furthermore, it also addresses the mapping of visual elements to sound, though the current work focuses on spatial audio rather than sonification.


\subsection{Audio-augmented museum experiences with gaze tracking}

Aiming to enrich the perception of landscape and genre paintings, Yang et al.~\cite{yang2019audio} track a visitor’s gaze and spatialize sounds for drawn objects and scenes within the paintings. Personalizing the audio output based on the user’s gaze, the system amplifies the sounds directed at the viewers focal point, attenuating the rest. 

Gaze and pose tracking required an eye-tracker and a connected laptop in a backpack. Additionally, headphones were used for spatial audio playback (Figure~\ref{fig:gaze-img}). The propagation of sound was dynamically simulated according to a user’s gaze and pose via the Google Resonance Audio SDK, while the Unity3D game engine was employed to model the room and map the various sound sources.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{gaze-1}
  \caption{Two of the paintings used in the application. Blue audio icons represent virtual sounds accurately spatialized relative to the user (b). (Adapted from~\cite{yang2019audio}).}
  \label{fig:gaze-img}
\end{figure}

A user study with 14 young adults revealed some challenges regarding the consistency of the eye-tracking and differences in preference across individuals, such as the amplification of sound and the smoothness of its adjustment. The experience was still positively received overall, as it helped most users focus on areas of interest, some even feeling guided by their gaze.

While gaze tracking proves itself a mostly intuitive approach to dynamic audio spatialization and interactivity with an artwork, it is not the most appropriate technique in the context of our work, which focuses on BVI users.

Nonetheless, this study provides valuable insights to our own. Akin to our proposal’s proximity-based 3D audio, it narrates a painting’s visual elements by spatially embedding sounds to specific points of interest and dynamically adjusting their intensities. Furthermore, there is relevance in learning from the challenges highlighted by Yang et al., namely in ensuring smooth audio transitions, responsiveness and accomodating for some degree of personal preference, through personalization.


\section{Accessibility in Auditory Representations of Art and Virtual Environments}

Objective: Discuss efforts to make art and virtual environments accessible through auditory means.

Topics to Include:
Audio descriptions of visual art and their focus on interpretation and historical context.
Research on spatialized audio as a method to convey spatial and emotional dimensions of visual art.
Applications in virtual environments that support BVI navigation and exploration through sound.
Studies comparing the effectiveness of auditory-only interfaces to multisensory approaches.

Link to Dissertation: Identify gaps in auditory representation methods for BVI individuals, emphasizing the need for interactive, spatially dynamic solutions.

\subsection{Navmol}

Navmol~\cite{fartaria2013navmol} is a molecular browser and editor specifically designed for blind and visually impaired users, aiming to provide BVI accessibility in the higher education of chemistry. 

Via a speech synthesizer, it provides an auditory portrayal of the atomic composition of complex molecular structures. Such configurations are bidimensionally depicted using the analogue clock metaphor, consisting of mapping directions to the positions of a clock and well known among the BVI community. As some users retain some degree of sight, the program has a simple graphical interface (Figure~\ref{fig:navmol-img}) with them in mind, visually displaying the selected atoms and some molecular contours.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{navmol}
  \caption{Navmol's clock system, where carbon-3 has neighbors at 6 o'clock and 10 o'clock, respectively carbon-4 and carbon-2 (Adapted from~\cite{fartaria2013navmol}).}
  \label{fig:navmol-img}
\end{figure}

Rodrigues~\cite{rodrigues2015navmol} merged the simple clock analogy with the application of HRTFs on the auditory signal generated by the existing Navmol program at the time (version 2.0), in order to create realistic directional sound cues, perceived to derive from where the atom is positioned. Usability tests demonstrated the efficacy of this integration, with users achieving an average task accuracy of 95.7\% in identifying and navigating molecular structures.

Knowing that HRTF performance may greatly vary across users due to inter-individual differences in morphology, Rodrigues conducted a study on the performance of 53 distinct HRTF measurements. It confirmed a significant variation in performance across different HRTF datasets for individual users. The five most consistently well performing measurements – KEMAR, CIAIR, IRC05, IRC25 and IRC44, were then selected for use in one additional study, motivated by the significant variation in performance across different HRTF datasets for individual users. The necessity to allow users to select their preferred HRTF dataset was made apparent and Navmol was updated accordingly.

Rodrigues’ work very much aligns with the goals of this dissertation, as it highlights not only the efficacy of spatial audio (integrated through HRTFs) in the perception of spatial and structural information but also addresses the suitability (or lack thereof) of specific HRTFs to distinct users, allowing them to tailor their own experience to a degree.


\subsection{MusA}

Ahmetovic et al.~\cite{ahmetovic2021musa} proposed MusA intending to address the limitations of traditional artwork accessibility methods, such as audio guides. MusA is a mobile application that leverages AR to provide interactive and accessible descriptions of paintings to low vision visitors. These descriptions are structured into chapters, each representing a specific area of the artwork, and are linked to an image overlayed on the artwork with a contour highlighting the described section.

The application features artwork recognition via the mobile’s camera, interactive navigation across chapters, and touch-based overlays. It was specifically designed for users with some residual sight, presenting a clutter-free and to the point interface (Figure~\ref{fig:musa-img}) compatible with system magnifiers, enlarged fonts and adjustable contrast filters.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{musa}
  \caption{Some screens of the MusA app. The first two screens respectively correspond to chapter navigation and selection in its first iteration. The third screen corresponds to the second iteration's virtual mode (Adapted from~\cite{ahmetovic2021musa}).}
  \label{fig:musa-img}
\end{figure}

After an initial user testing with LV participants identifying some challenges, a second and final iteration of the app incorporated audio and haptic feedback, higher contrast contours, and a zoom-supportant virtual mode designed to replace AR in case the user can’t frame the painting continuously. 

User studies revealed MusA to be a significantly more engaging and user-friendly experience than a traditional audio guide, promoting freedom in exploration. Despite some issues in overlay clarity, MusA was effective in supporting the needs of low-vision people and they were pleased by the ability to use the app at home and in their own device.

The work of Ahmetovic et al. parallels that of this dissertation in certain aspects, as alike the chapter navigation available in MusA, the proposed interactive soundscape will offer users control over what details they wish to explore and focus on specific features of the artwork, only with proximity-based or gyroscopic interactions. Their findings show that even for low vision users, visual feedback through properly contrasting overlays enriches the perception of a painting’s structure and details. As such, we mustn’t undervalue visual cues and should ensure low visual clutter alongside proper visibility settings, such as contrast and font size.



\section{Leisurely and BVI Inclusive Applications}

Objective: Explore how gamification and accessible design principles have been used to engage BVI individuals in leisure and entertainment.

Topics to Include:
BVI-accessible video games incorporating spatial audio for navigation and storytelling.
Gamified applications that support learning or cultural engagement for BVI users.
Examples of audio-only games and their design considerations for accessibility.

Link to Dissertation: Highlight how gamification principles and interactive audio can enhance both accessibility and user engagement in art-related experiences.

\section{Tools for Soundscape Creation}

Objective: Review existing tools and frameworks for creating soundscapes, with a focus on their accessibility and ease of use.

Topics to Include:
Software tools used for designing soundscapes (e.g., DAWs with spatial audio plugins, dedicated soundscape editors).
Approaches for integrating spatial audio and interactivity into soundscape creation.
Evaluation of user-friendliness and adaptability of these tools for non-experts, such as museum curators.

Link to Dissertation: Emphasize the need for intuitive, accessible tools tailored to curators and the creation of high-quality, interactive soundscapes.

\section{Summary and Final Design Considerations}

Objective: Provide a synthesis of the reviewed work, highlighting gaps and considerations relevant to your proposed solution.

Topics to Include:
Key limitations of existing immersive audio systems and accessibility efforts.
Opportunities for innovation in soundscape design and spatial audio accessibility.
Final considerations for designing a tool that balances accessibility, interactivity, and usability for both creators and BVI users.

Link to Dissertation: Summarize how your proposed solution builds upon and addresses the shortcomings identified in prior research.
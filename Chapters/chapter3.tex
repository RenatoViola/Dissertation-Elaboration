%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{Related Work}
\label{cha:related-work}

% epigraph configuration
\epigraphfontsize{\small\itshape}
\setlength\epigraphwidth{12.5cm}
\setlength\epigraphrule{0pt}

\epigraph{
  This chapter reviews relevant research and applications mostly related to immersive spatial audio and BVI accessibility, including technologies and approaches that have been used to address said accessibility. It is divided by sections, each of which focusing on a specific topic related to the dissertation's theme. Each of these sections starts with a brief overview of related studies and projects displaying the current state of the art. It is then followed by subsections where projects that are of particular relevance to this dissertation are explored in some detail, from implementation to findings, and finally how they relate to our work and what is to be learned from them.
}{}

\section{Immersive Audio Experiences in Cultural Environments}

Audio is a widely used vehicle for delivering immersive experiences in cultural environments. The following two studies exemplify this statement, and are centered around spatial audio, since it is a central theme to this dissertation. 

Focused on recreating the city of Évora’s culturally rich historical soundscapes, Ferreira~\cite{ferreira2021creating} created Immerscape, a tool aimed at non-expert users for generating 3D audio scenes, utilizing HRTFs to spatialize sound. We cover Immerscape in further detail in subsection~\ref{ssec:immerscape}. Kabisch et al.~\cite{kabisch2005sonic} used motion tracking, image analysis and sonification alongside real-time directional sound to integrate panoramic visual landscapes with spatialized audio, presenting such research in an interactive art exhibit.

\subsection{Eyes-Free Art}

Looking to enhance the accessibility of visual art to BVI individuals and go beyond the shortcomings of the typical audio descriptions or guides, Rector et al.~\cite{rector2017eyes} designed “Eyes-Free Art”, a novel approach to sonically interacting with 2D art, aiming to be both aesthetically stimulating and engaging. 

It is in essence, a carefully crafted proxemic audio interface that, mirroring the conventional intuition of visual proxemic interfaces, renders more detail as proximity to the piece increases. To define such proximity, a Microsoft Kinect device was used to track the user’s position and movements, not only to determine the user’s distance to the painting but also if it is facing towards it. The audio interpretation a user hears varies according to the proxemic zone in which it finds itself, of which there are four distinct and equally sized ones (Figure~\ref{fig:eyes-free-art-img}). Upon entering any zone, the user is verbally alerted to where it is and may continue moving between zones, spending as much time as it wants in each.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{eyes-free-art}
  \caption{The four zones of the Eyes-Free Art proxemical interface. Image is \textit{The Blue Rider} from Wassily Kandinsky (Reproduced from~\cite{rector2017eyes}). \copyright~2017 Copyright held by the owner/author(s).}
  \label{fig:eyes-free-art-img}
\end{figure}

The furthest zone consists only of background music used to set the mood of the piece. It is followed by a sonification area aimed at communicating the painting’s chromatic diversity through musical features. The second-closest zone, sound effects, highlights the painting’s literal aspects such as the type of objects and their spatial correlation. The final and most detailed of zones consists of a manually curated verbal description. 

Some initial interviews were conducted, from which Rector et al. noted the importance of using commodity technology (promoting control and independence) and including both the literal and subjective aspects of a painting.

A final evaluation with 13 BVI participants attested to the success of this implementation, as patrons felt immersed and had a rich experience interpreting the art work.

Eyes-Free Art resonates with the current dissertation in several ways. Most importantly, in its integration of zones of differing detail according to proximity, allowing users to explore at their own pace and at their desired level of detail, closely aligning with our goal of promoting independence and interactivity. Furthermore, it also addresses the mapping of visual elements to sound, though the current work focuses on spatial audio rather than sonification.


\subsection{Audio-augmented museum experiences with gaze tracking}

Aiming to enrich the perception of landscape and genre paintings, Yang et al.~\cite{yang2019audio} track a visitor’s gaze and spatialize sounds for drawn objects and scenes within the paintings. Personalizing the audio output based on the user’s gaze, the system amplifies the sounds directed at the viewers focal point, attenuating the rest. 

Gaze and pose tracking required an eye-tracker and a connected laptop in a backpack. Additionally, headphones were used for spatial audio playback (Figure~\ref{fig:gaze-img}). The propagation of sound was dynamically simulated according to a user’s gaze and pose via the Google Resonance Audio SDK, while the Unity3D game engine was employed to model the room and map the various sound sources.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{gaze-1}
  \caption{Two of the paintings used in the application. Blue audio icons represent virtual sounds accurately spatialized relative to the user (b). (Adapted from~\cite{yang2019audio}). \copyright~2019 Copyright held by the owner/author(s).}
  \label{fig:gaze-img}
\end{figure}

A user study with 14 young adults revealed some challenges regarding the consistency of the eye-tracking and differences in preference across individuals, such as the amplification of sound and the smoothness of its adjustment. The experience was still positively received overall, as it helped most users focus on areas of interest, some even feeling guided by their gaze.

While gaze tracking proves itself a mostly intuitive approach to dynamic audio spatialization and interactivity with an artwork, it is not the most appropriate technique in the context of our work, which focuses on BVI users.

Nonetheless, this study provides valuable insights to our own. Akin to our proposal’s proximity-based 3D audio, it narrates a painting’s visual elements by spatially embedding sounds to specific points of interest and dynamically adjusting their intensities. Furthermore, there is relevance in learning from the challenges highlighted by Yang et al., namely in ensuring smooth audio transitions, responsiveness and accommodating for some degree of personal preference, through personalization.


\section{BVI Accessibility in Multisensory Experiences}

Multisensory experiences have been shown to have potential in improving accessibility and independence for the blind and visually impaired, by providing alternative ways of perceiving content considered to be visual in essence.

Li~\cite{li2024beyond} developed an audio-only inclusive prototype for navigating AR content without having to rely on visual cues, by incorporating spatialized audio to provide intuitive feedback on object proximity and spatial relationships. Meanwhile, Cavazos Quero et al.~\cite{cavazos2021accessible} implemented a touch-sensitive multimodal guide providing localized audio descriptions based on touch, promoting independence in both the exploration and interpretation of artworks. Banf and Blanz~\cite{banf2013sonification} presented a system using touchscreens that actively promotes a visually impaired user to explore an image and receive audio feedback corresponding to its local content. Such was achieved by employing computer vision and machine learning algorithms to sonify image features from a low level, such as color and texture, to  high level object recognition.

\subsection{Navmol}

Navmol~\cite{fartaria2013navmol} is a molecular browser and editor specifically designed for blind and visually impaired users, aiming to provide BVI accessibility in the higher education of chemistry. 

Via a speech synthesizer, it provides an auditory portrayal of the atomic composition of complex molecular structures. Such configurations are bidimensionally depicted using the analogue clock metaphor, consisting of mapping directions to the positions of a clock and well known among the BVI community. As some users retain some degree of sight, the program has a simple graphical interface (Figure~\ref{fig:navmol-img}) with them in mind, visually displaying the selected atoms and some molecular contours.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{navmol}
  \caption{Navmol's clock system, where carbon-3 has neighbors at 6 o'clock and 10 o'clock, respectively carbon-4 and carbon-2 (Adapted from~\cite{fartaria2013navmol}).}
  \label{fig:navmol-img}
\end{figure}

Rodrigues~\cite{rodrigues2015navmol} merged the simple clock analogy with the application of HRTFs on the auditory signal generated by the existing Navmol program at the time (version 2.0), in order to create realistic directional sound cues, perceived to derive from where the atom is positioned. Usability tests demonstrated the efficacy of this integration, with users achieving an average task accuracy of 95.7\% in identifying and navigating molecular structures.

Knowing that HRTF performance may greatly vary across users due to inter-individual differences in morphology, Rodrigues conducted a study on the performance of 53 distinct HRTF measurements. It confirmed a significant variation in performance across different HRTF datasets for individual users. The five most consistently well performing measurements – KEMAR, CIAIR, IRC05, IRC25 and IRC44, were then selected for use in one additional study, motivated by the significant variation in performance across different HRTF datasets for individual users. The necessity to allow users to select their preferred HRTF dataset was made apparent and Navmol was updated accordingly.

Rodrigues’ work very much aligns with the goals of this dissertation, as it highlights not only the efficacy of spatial audio (integrated through HRTFs) in the perception of spatial and structural information but also addresses the suitability (or lack thereof) of specific HRTFs to distinct users, allowing them to tailor their own experience to a degree.


\subsection{MusA}

Ahmetovic et al.~\cite{ahmetovic2021musa} proposed MusA intending to address the limitations of traditional artwork accessibility methods, such as audio guides. MusA is a mobile application that leverages AR to provide interactive and accessible descriptions of paintings to low vision visitors. These descriptions are structured into chapters, each representing a specific area of the artwork, and are linked to an image overlaid on the artwork with a contour highlighting the described section.

The application features artwork recognition via the mobile’s camera, interactive navigation across chapters, and touch-based overlays. It was specifically designed for users with some residual sight, presenting a clutter-free and to the point interface (Figure~\ref{fig:musa-img}) compatible with system magnifiers, enlarged fonts and adjustable contrast filters.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{musa}
  \caption{Some screens of the MusA app. The first two screens respectively correspond to chapter navigation and selection in its first iteration. The third screen corresponds to the second iteration's virtual mode (Adapted from~\cite{ahmetovic2021musa}). \copyright~2021 Copyright held by the owner/author(s).}
  \label{fig:musa-img}
\end{figure}

After an initial user testing with LV participants identifying some challenges, a second and final iteration of the app incorporated audio and haptic feedback, higher contrast contours, and a zoom-supportant virtual mode designed to replace AR in case the user can’t frame the painting continuously. 

User studies revealed MusA to be a significantly more engaging and user-friendly experience than a traditional audio guide, promoting freedom in exploration. Despite some issues in overlay clarity, MusA was effective in supporting the needs of low-vision people, and they were pleased by the ability to use the app at home and in their own device.

The work of Ahmetovic et al. parallels that of this dissertation in certain aspects, as alike the chapter navigation available in MusA, the proposed interactive soundscape will offer users control over what details they wish to explore and focus on specific features of the artwork, only with proximity-based or gyroscopic interactions. Their findings show that even for low vision users, visual feedback through properly contrasting overlays enriches the perception of a painting’s structure and details. As such, we mustn’t undervalue visual cues and should ensure low visual clutter alongside proper visibility settings, such as contrast and font size.



\section{Leisurely BVI Inclusive Applications}

Leisurely applications such as games designed for BVI individuals aim to merge entertainment with accessibility by integrating multisensory technologies such as spatial audio and haptics, among others. Their goal is to provide experiences that are as unique as they are inclusive.

Nair et al.~\cite{nair2021navstick} introduced a spatial audio-based navigation tool for 3D games, enabling BVI players to independently explore and create their own mental maps of virtual worlds. Navstick uses HRTFs for spatialized sound and allows users to scan the contents of their vicinity in specific directions using a thumbstick. Furthermore, Navstick is the directional scanner SAT employed in subsection~\ref{ssec:sats}, and yielded positive feedback.
Sánchez and Sáenz~\cite{sanchez2007usability} designed and evaluated the usability of three distinct interactive 3D virtual environments for visually impaired learners: AudioMUD, AudioVida and AudioChile. Such environments were navigatable and interacted with through sound, their spatiality conveyed by spatialized audio and described by voiced narration.
Simão~\cite{simao2018jogo} proposed B\textsc{lind} A\textsc{dventure}, a mobile audio game aimed at training the orientation and mobility skills of visually impaired children, by presenting several challenges requiring physical movement, tracked through the device’s sensors. The game’s virtual environments were built with the Unity game engine and featured localizable sound cues, implemented through HRTFs derived from the SADIE database.


\subsection{LEAP Tic-Tac-Toe}

Drossos et al.~\cite{drossos2015accessible} adapted an audio-only version of Tic-Tac-Toe to be made both appealing and accessible to visually impaired children, by empowering it with sonic displays.

Simple as it is, Tic-Tac-Toe is a visually reliant game and spatial perception alongside game state, positioning and rules all posed a significant challenge when designing it to be BVI accessible. Additionally, people with residual vision must also be accounted for, enabling them to complement their auditory experience with their remaining vision.

To this end, they designed and implemented not only the game itself but also a custom game engine and audio engine specifically intended at the effective development of audio games accessible to the visually impaired. Within the game world represented by the game engine, static objects are the most common type of game object, in which sound objects are included.

Carefully ensuring that BVI auditory needs were met, Drossos et al. categorized sound objects into three types that when collectively employed, enable the development of any stage, be it simple or complex. These are classified as: Standard (no interaction with the game, ambient soundtrack is an example), Interruptible (sounds that provide little more than redundant or aesthetic information) and Blocking (sounds carrying important information requiring special attention, interruptible sounds are discarded).

The audio engine functions as the API serving the audio to these objects and handles the sound settings as well as the sonic display implementation. Auditory icons (recognizable real-world sounds) and earcons (synthesic sounds) are used for game state awareness while binaural processing (utilizing the KEMAR HRTF library) is used for conveying spatial details.

The game’s GUI is simple and employs intense contrast alongside large optical elements (Figure~\ref{fig:tic-tac-toe-img}), accomodating for users with residual sight. The auditory interface provides constant feedback of the effects for every action and players sense direction and three-dimensionality through the localized sound cues. In addition, there are pre-recorded audio instructions for each game component.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{tic-tac-toe}
  \caption{Part of the Tic-Tac-Toe game's graphical user interface (Adapted from~\cite{drossos2015accessible}). \copyright~2015 ACM.}
  \label{fig:tic-tac-toe-img}
\end{figure}

In user studies, visually impaired children received the game very positively, though many were experiencing an accessible game for the first time.

Drossos et al.’s findings are of value for this dissertation as they address challenges we will inevitably come across such as the need for constant, concurrent and discernible audio feedback, as well as conveying localization information through the sounds in order for users to navigate themselves around the soundscape. Additionally, they also address relevant limitations as in not all sounds being equally effective for spatial localization and the varying spatial awareness among users.


\subsection{The Preferred Spatial Awareness Tools for BVI People In Video Games}
\label{ssec:sats}

While the minimap is one of the most employed spatial awareness tools (SATs) in video games, crucial even for sighted players in learning the layout of their surroundings, it still has no successful equivalent in regards to BVI accessibility. 

Attempting to bridge this gap in acessibility, Nair et al.~\cite{nair2021towards} took it upon themselves to tackle the creation of a universal and acoustic BVI-friendly minimap, or more concretely, uncovering the most relevant design factors as well as the merits and limitations of the best acoustic techniques to do so.

Two main questions were at the center of their study, the first regarding the key aspects of spatial awareness valued the most by visually impaired players in games, and the second focusing on the effectiveness of current SATs in supporting said aspects.

Intending to delve into both, Nair et al.~\cite{nair2022uncovering} investigate the design of the four leading SATs (Figure~\ref{fig:dungeon-escape-img}) in enhancing spatial perception for the visually impaired in a 3D game world, vastly different approaches developed in previous research. These being the: 
\begin{itemize}
  \item \textit{Smartphone Map -} A touchscreen map working in tandem with the game to provide spatial information through sound effects and text-to-speech
  \item \textit{Whole-Room Shockwave –} When triggered, emits 3D sounds from certain objects based on their distance, simulating a refined and customizable echolocation
  \item \textit{Directional Scanner –} Allows players to survey a surrounding direction by tilting the right thumbstick towards it, announcing the first object in line-of-sight with directional sound
  \item \textit{Simple Audio Menu –} Lists the points of interest in a room through their corresponding sounds effects and text-to-speech, in alphabetical order
\end{itemize}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{dungeon-escape}
  \caption{The four spatial awareness tools implemented within Dungeon Escape~\cite{nair2022uncovering}). \copyright~2022 ACM.}
  \label{fig:dungeon-escape-img}
\end{figure}

To evaluate the effectiveness of each of these approaches in conveying the most essential aspects of spatial awareness and address the two main questions of the research, Nair et al. implemented them all into Dungeon Escape, an original third person 3D game in which the players were required to use the given SATs to gain enough spatial awareness to succeed. 

Implemented in the Unity game engine, Dungeon Escape is an adventure game specifically designed for the study and consists in players navigating dungeons in search of objects that allow them to overcome obstacles and thus escape the dungeons. 

Despite its focus being on studying the performance of the SATs within the differing dungeon layouts, it is more than a simple playground for the different SATs, portraying itself as an accessible game in other ways. As in well known 3D games, the left thumbstick is used for movement and rotation, and in this case aided by an utility mimicking snap rotation. Collisions have an unique sound effect and relevant sound plays from any object within a 2-meter radius of the player. Furthermore, players can lock onto objects of interest by placing a looping audio beacon on them.

Following a user study with 9 participants, of which eight were completely blind, the most critical aspect of spatial awareness was found to be position and orientation. Presence, arrangement and adjacent areas were tied for second place, whilst shape and scale overwhelmingly came last. 

One of the key findings was that despite the importance of position and orientation, none of the approaches were fully satisfactory, though the directional scanner performed the best. One other relevant takeaway was the effectiveness of combining some of the SATs, with the greatest spatial awareness being provided by the directional scanner and simple audio menu combination. Finally, visually impaired individuals greatly value customizable SATs, as the aforementioned SAT combination was closely followed by the combination of the directional scanner and whole-room shockwave. This is despite the whole-room shockwave having been considered overwhelming in several instances (yet customizable), and the simple audio menu a great way to communicate presence (yet disliked by half the participants for being “spoilery”).

Nair et al.’s research provides several insights to the current dissertation mostly by exposing the strengths and weaknesses of the different SATs but also in the accessible design of Dungeon Escape. Most importantly, it highlights the importance of position and orientation for proper spatial understanding and exploration, which we will prioritize conveying in our work, possibly by incorporating combinations of the most synergetic SATs as was done in their study. Additionally, it touches on how users may be overwhelmed by too much detail, inspiring us to focus on providing cues qualitatively rather than quantitatively.





\section{Tools for Soundscape Creation}

The current market is home to advanced soundscape creation software, enabling creators to design highly detailed and expressive auditory environments tailored to the most diverse applications. Different tools have different purposes, with some focusing on professional sound design, where the spatial accuracy of sound environments is the priority, whilst others aim for dynamism and adaptability.

In the matter of sound design and immersive spatial audio, Dolby Atmos~\cite{dolby-atmos} stands out as the industry’s standard in delivering 3D audio experiences, be it in entertainment, gaming or VR. In Dolby Atmos~\cite{dolby-atmos}, environment creation is object-based, meaning that sounds may be placed and moved as discrete objects in a 3D space. Sound Particles~\cite{sound-particles} is one other tool meriting a mention, as it too delivers immersive high quality audio for films, gaming and virtual environments, excelling on large-scale 3D audio effects by simulating spatialized sound with particle systems.

Though not exclusively centered around sound design and production, game engines such as Unity~\cite{unity} and Unreal Engine~\cite{unreal-engine} have proven to be effective platforms for soundscape creation, offering robust audio capabilities and integrations. Unity~\cite{unity}, for instance, provides built-in spatial sound support through its default audio system, with more advanced spatial audio capabilities being made available through integration with third-party tools like FMOD~\cite{fmod} and Google’s Resonance Audio~\cite{resonance-audio}. Unreal Engine~\cite{unreal-engine} features a built-in audio engine supporting 3D spatialization, reverberation effects and audio triggers, more advanced than Unity’s~\cite{unity} default audio system but not accounting for its third-party augmentations.

As expressive and competent as the tools mentioned so far are, there is a significant learning curve to them, a tradeoff for such feature richness and customization ability. In the context of this dissertation, where the process of soundscape creation shouldn’t require prior experience in sound design, an adequate balance between expressiveness and complexity is a must. Additionally, there are certain design considerations we should adhere to or at least consider regarding the visually impaired, addressed in certain research.

Guerreiro et al.~\cite{guerreiro2023design} proposed a theoretical framework to support BVI inclusive auditory representations of object location, behavior and interaction virtual environments, utilizing spatialized sound and sonification in Unity. Their proposed design space explored nine distinct categories, such as the audio field, concurrency and spatialization. A user study they conducted showed that sound spatialization wasn’t always preferred, particularly when dealing with moving objects, and that fully concurrent auditory feedback could be mentally overwhelming. Meanwhile, Krol et al.~\cite{krol2024design} investigated the use of automated musical soundscapes in visual art accessibility for the visually impaired. From their study, several design considerations were highlighted: enhancing narrative comprehension by integrating storytelling elements; soundscape customization options; integration with other accessibility methods such as audio descriptions; contextual and historical accuracy/adequacy; inclusion of ambient sound effects; effective audio reproduction such as spatial sound and reverb to convey location, movement and atmosphere.


\subsection{Immerscape}
\label{ssec:immerscape}

Contributing to the PASEV project, focused on retaining and promoting the city of Évora’s cultural patrimony, namely the rich soundscapes correspondent to the various historical events which took place between 1540 and 1910, Ferreira~\cite{ferreira2021creating} proposed Immerscape.

Intended to provide the PASEV projects’s team with the means to reconstruct the city’s auditory history from current sound recordings, Immerscape is a soundscape editing tool accessible even to those without prior experience in programming or sound composition software, allowing for the creation of historical soundscapes out of previously collected recordings and the generation of immersive 3D audio files representing such soundscapes, while abstracting away the technical details behind spatial audio generation.

The editor itself was implemented within the Unity game engine and the acoustic immersion leveraged the Google Resonance Audio SDK to spatialize sound through the application of HRTF filters, requiring the use of headphones to be properly evaluated. 

Besides an accessible interface with minimal complexity (Figure~\ref{fig:immerscape-img}), Ferreira defined some fundamental requirements for Immerscape such as the ability to select predefined 3D sound environments (with unique resonance and reverb properties), immersive environmental navigation, creation of editable sound sources (audio properties, movement and triggering events) and real-time playback/recording of the soundscape. Additionally, there are two available camera angles in development, the default being the player view and the alternative an up-view.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{immerscape}
  \caption{Immerscape's environment, through the player view perspective. The red cube represents an audio object (Reproduced from~\cite{ferreira2021creating}). \copyright~2021 Carolina Ribeiro Dias Ferreira.}
  \label{fig:immerscape-img}
\end{figure}

An evaluation with 10 developer and 7 non-developer participants highlighted Immerscape’s high usability and immersive quality.
It is one of many examples where spatialized audio with the HRTF technique was proven to be an effective way of providing engaging sonic experiences, in this case by immersively conveying historical soundscapes. 

While Ferreira’s work targets general audiences rather than the BVI demographic, there is much to take away from it in the context of this dissertation.
Immerscape’s design as well as its implementation of the HRTF filters is undoubtebly an inspiration to what will transpire over the course of our work, particularly regarding spatial audio simulation and ease of use for non-experienced users. Furthermore, some of the requirements defined by Ferreira including predefined 3D sound environments, editable audio sources and real-time playback of the scene are likely to be imported over to our research, aligning with our goal of creating an intuitive yet expressive soundscape editor.
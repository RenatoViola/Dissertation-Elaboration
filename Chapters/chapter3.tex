\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{Related Work}
\label{cha:related-work}

\epigraphfontsize{\small\itshape}
\setlength\epigraphwidth{12.5cm}
\setlength\epigraphrule{0pt}

\epigraph{
  This chapter reviews relevant research and applications mainly related to immersive spatial audio and \gls{BVI} accessibility, including technologies and approaches to address said accessibility. It is divided into sections, each focusing on a specific topic related to the dissertation's theme. Each section starts with a brief overview of related studies and projects displaying the current state of the art. It is then followed by subsections where projects of particular relevance to this dissertation are explored in detail, from implementation to findings, and finally, how they relate to the current solution and what is to be learned from them.
}{}

\section{Immersive Audio Experiences in Cultural Environments}

Audio is a widely used vehicle for delivering immersive experiences in cultural environments. The following two studies exemplify this statement and are centered around spatial audio since it is a central theme of this dissertation. 

Focused on recreating the city of Évora’s culturally rich historical soundscapes, Ferreira~\cite{ferreira2021creating} created Immerscape, a tool aimed at non-expert users for generating \gls{3D} audio scenes, utilizing \gls{HRTF}s to spatialize sound. Immerscape is covered in further detail in subsection~\ref{ssec:immerscape}. Kabisch et al.~\cite{kabisch2005sonic} used motion tracking, image analysis, and sonification alongside real-time directional sound to integrate panoramic visual landscapes with spatialized audio, presenting such research in an interactive art exhibit.

\subsection{Eyes-Free Art}

Looking to enhance the accessibility of visual art to \gls{BVI} individuals and go beyond the shortcomings of the typical audio descriptions or guides, Rector et al.~\cite{rector2017eyes} designed Eyes-Free Art. Eyes-Free Art was a novel approach to sonically interacting with \gls{2D} art, aiming to be both aesthetically stimulating and engaging. 

It is a carefully crafted proxemic audio interface that, mirroring the conventional intuition of visual proxemic interfaces, renders more detail as proximity to the piece increases. In order to define such proximity, a Microsoft Kinect device was used to track the user's position and movements to determine the distance to the painting and if it is facing toward it. The audio interpretation a user hears varies according to the proxemic zone in which it finds itself, of which there are four distinct and equally sized ones (Figure~\ref{fig:eyes-free-art-img}). Upon entering any zone, the user is verbally alerted to where it is and may continue moving between zones, spending as much time as he/she wants in each.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{eyes-free-art}
  \caption{The four zones of the Eyes-Free Art proxemical interface. Image is \textit{The Blue Rider} from Wassily Kandinsky (Reproduced from~\cite{rector2017eyes}). \copyright~2017 Copyright held by the owner/author(s).}
  \label{fig:eyes-free-art-img}
\end{figure}

The furthest zone consists only of background music that sets the piece's mood. It is followed by a sonification area that communicates the painting's chromatic diversity through musical features. The second-closest zone, sound effects, highlights the painting's literal aspects, such as the type of objects and their spatial correlation. The final and most detailed zone consists of a manually curated verbal description. 

Some initial interviews were conducted, from which Rector et al. noted the importance of using commodity technology (promoting control and independence) and including both the literal and subjective aspects of a painting. A final evaluation with 13 \gls{BVI} participants attested to the success of this implementation, as patrons felt immersed and had a rich experience interpreting the artwork.

Eyes-Free Art resonates with the current dissertation in several ways. Most importantly, it integrates zones of differing detail according to proximity, allowing users to explore at their own pace and at their desired level of detail. This closely aligns with this dissertation's goal of promoting independence and interactivity. Furthermore, it also addresses mapping visual elements to sound, though the current work focuses on spatial audio rather than sonification.


\subsection{Audio-augmented museum experiences with gaze tracking}

Aiming to enrich the perception of landscape and genre paintings, Yang et al.~\cite{yang2019audio} track a visitor’s gaze and spatialize sounds for drawn objects and scenes within the paintings. The system personalizes the audio output based on the user’s gaze; the system amplifies the sounds directed at the viewer's focal point, attenuating the rest. 

Gaze and pose tracking required an eye tracker and a laptop connected to a backpack. Additionally, headphones were used for spatial audio playback (Figure~\ref{fig:gaze-img}). Sound propagation was dynamically simulated according to a user’s gaze and pose via the Google Resonance Audio SDK\footnote{https://resonance-audio.github.io/resonance-audio/}. At the same time, the Unity\footnote{https://unity.com/} game engine was employed to model the room and map the various sound sources.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{gaze-1-cropped}
  \caption{Two of the paintings used in the application. Blue audio icons represent virtual sounds accurately spatialized relative to the user (b). (Adapted from~\cite{yang2019audio}). \copyright~2019 Copyright held by the owner/author(s).}
  \label{fig:gaze-img}
\end{figure}

A user study with 14 young adults revealed some challenges regarding the consistency of eye-tracking and differences in preference across individuals, such as the amplification of sound and the smoothness of its adjustment. Overall, the experience was still positively received, as it helped most users focus on areas of interest, some even feeling guided by their gaze.

While gaze tracking is a mostly intuitive approach to dynamic audio spatialization and interactivity with artwork, it is not the most appropriate technique for this work, which focuses on \gls{BVI} users. Nonetheless, this study provides valuable insights to the current research. Akin to the proposal’s proximity-based \gls{3D} audio, it narrates a painting’s visual elements by spatially assigning sounds to specific points of interest and dynamically adjusting their intensities. Finally, there is relevance in learning from the challenges faced by Yang et al., especially in accommodating for personal preference through customization.


\section{BVI Accessibility in Multisensory Experiences}

Multisensory experiences have been shown to have the potential to improve accessibility and independence for the blind and visually impaired by providing alternative ways of perceiving essentially visual content. 

Li~\cite{li2024beyond} developed an audio-only inclusive prototype for navigating \gls{AR} content without relying on visual cues, incorporating spatialized audio to provide intuitive feedback on object proximity and spatial relationships. Meanwhile, Cavazos Quero et al.~\cite{cavazos2021accessible} implemented a touch-sensitive multimodal guide providing localized audio descriptions based on touch, which promoted independence in both the exploration and interpretation of artworks. Banf and Blanz~\cite{banf2013sonification} presented a system using touchscreens that allow a visually impaired user to explore an image and receive audio feedback corresponding to its local content. This procedure employed computer vision and machine learning algorithms to sonify image features from low levels, such as color and texture, to high-level object recognition.

\subsection{Navmol}

Navmol~\cite{fartaria2013navmol} is a molecular browser and editor specifically designed for blind and visually impaired users, aiming to provide \gls{BVI} accessibility in higher chemistry education. Via a speech synthesizer, it provides an auditory portrayal of the atomic composition of complex molecular structures. Such configurations are bidimensionally depicted using the analog clock metaphor, consisting of mapping directions to the positions of a clock, and are well known among the \gls{BVI} community. As some users retain some degree of sight, the program has a simple graphical interface (Figure~\ref{fig:navmol-img}) with them in mind, visually displaying the selected atoms and some molecular contours.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{navmol}
  \caption{Navmol's clock system, where carbon-3 has neighbors at 6 o'clock and 10 o'clock, respectively carbon-4 and carbon-2 (Adapted from~\cite{fartaria2013navmol}).}
  \label{fig:navmol-img}
\end{figure}

Rodrigues~\cite{rodrigues2015navmol} merged the simple clock analogy with the application of \gls{HRTF}s on the auditory signal generated by the existing Navmol program at the time (version 2.0) in order to create realistic directional sound cues, perceived to derive from where the atom is positioned. Usability tests demonstrated the efficacy of this integration, with users achieving an average task accuracy of 95.7\% in identifying and navigating molecular structures.

Knowing that \gls{HRTF} performance may greatly vary across users due to inter-individual morphological differences, Rodrigues conducted a study on the performance of 53 distinct \gls{HRTF} measurements. It confirmed a significant variation in performance across different \gls{HRTF} datasets for individual users. The five most consistently well-performing measurements – KEMAR, CIAIR, IRC05, IRC25, and IRC44, were then selected for use in one additional study, motivated by the significant variation in performance across different \gls{HRTF} datasets for individual users. The necessity of allowing users to select their preferred \gls{HRTF} dataset was apparent, and Navmol was updated accordingly.

Rodrigues’ work very much aligns with the goals of this dissertation, as it highlights not only the efficacy of spatial audio (integrated through \gls{HRTF}s) in the perception of spatial and structural information but also addresses the suitability (or lack thereof) of specific \gls{HRTF}s to distinct users, allowing them to tailor their own experience to a degree.


\subsection{MusA}

Ahmetovic et al.~\cite{ahmetovic2021musa} proposed MusA intending to address the limitations of traditional artwork accessibility methods, such as audio guides. MusA is a mobile application that leverages \gls{AR} to provide interactive and accessible descriptions of paintings to low-vision visitors. These descriptions are structured into chapters, each representing a specific artwork area. They are linked to an image overlaid on the artwork with a contour highlighting the described section.

The application features artwork recognition via the mobile's camera, interactive navigation across chapters, and touch-based overlays. It was designed for users with some residual sight, presenting a clutter-free and to-the-point interface (Figure~\ref{fig:musa-img}) compatible with system magnifiers, enlarged fonts, and adjustable contrast filters.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{musa-cropped}
  \caption{Some screens of the MusA app. The first two screens respectively correspond to chapter navigation and selection in its first iteration. The third screen corresponds to the second iteration's virtual mode (Adapted from~\cite{ahmetovic2021musa}). \copyright~2021 Copyright held by the owner/author(s).}
  \label{fig:musa-img}
\end{figure}

After an initial user testing with low-vision participants identifying some challenges, a second and final iteration of the app incorporated audio and haptic feedback, higher contrast contours, and a zoom-supporting virtual mode designed to replace \gls{AR} if the user cannot frame the painting continuously. User studies revealed MusA to be a significantly more engaging and user-friendly experience than a traditional audio guide, promoting freedom in exploration. Despite some issues in overlay clarity, MusA was effective in supporting the needs of low-vision people, and they were pleased by the ability to use the app at home and on their own devices.

The work of Ahmetovic et al. parallels that of this dissertation in certain aspects, like the chapter navigation available in MusA. The proposed interactive soundscape offers users control over what details they wish to explore and focus on specific artwork features only with proximity-based interactions. Their findings show that visual feedback through properly contrasting overlays enriches the perception of a painting's structure and details, even for low-vision users. They also show that there exists an appreciation for remote artistic experiences. From Ahmetovic et al.'s research, it was derived that visual cues should not be undervalued, and that the current solution should ensure low visual clutter alongside proper visibility settings, such as contrast and font size.



\section{Leisurely BVI Inclusive Applications}

Leisurely applications such as games designed for \gls{BVI} individuals aim to merge entertainment with accessibility by integrating multisensory technologies such as spatial audio and haptics. Their goal is to provide experiences as unique as they are inclusive.

Nair et al.~\cite{nair2021navstick} introduced a spatial audio-based navigation tool for \gls{3D} games, enabling \gls{BVI} players to explore and create their mental maps of virtual worlds independently. Navstick uses \gls{HRTF}s for spatialized sound and allows users to scan the contents of their vicinity in specific directions using a thumbstick. Furthermore, Navstick is the directional scanner \gls{SAT} employed in subsection~\ref{ssec:sats}, and yielded positive feedback.
Sánchez and Sáenz~\cite{sanchez2007usability} designed and evaluated the usability of three distinct interactive \gls{3D} virtual environments for visually impaired learners: AudioMUD, AudioVida, and AudioChile. Such environments were navigatable and interacted with through sound, their spatiality conveyed by spatialized audio and described by voiced narration.
Simão~\cite{simao2018jogo} proposed B\textsc{lind} A\textsc{dventure}, a mobile audio game aimed at training the orientation and mobility skills of visually impaired children by presenting several challenges requiring physical movement, tracked through the device’s sensors. The game’s virtual environments were built with the Unity game engine and featured localizable sound cues implemented through \gls{HRTF}s derived from the SADIE database.


\subsection{LEAP Tic-Tac-Toe}

Drossos et al.~\cite{drossos2015accessible} adapted an audio-only version of Tic-Tac-Toe to be made appealing and accessible to visually impaired children by empowering it with sonic displays. Simple as it is, Tic-Tac-Toe is a visually reliant game, and spatial perception, game state, positioning, and rules all posed a significant challenge when designing it to be \gls{BVI} accessible. Additionally, people with residual vision must be accounted for, enabling them to complement their auditory experience with their remaining vision.

To this end, they designed and implemented not only the game itself but also a custom game engine and audio engine specifically intended for the effective development of audio games accessible to the visually impaired. Within the game world represented by the game engine, static objects are the most common type of game object, and sound objects are included.

Carefully ensuring that \gls{BVI} auditory needs were met, Drossos et al. categorized sound objects into three types that, when collectively employed, enable the development of any stage, be it simple or complex. These are classified as Standard (no interaction with the game; ambient soundtrack is an example), Interruptible (sounds that provide little more than redundant or aesthetic information), and Blocking (sounds carrying important information requiring special attention; interruptible sounds are discarded).

The audio engine functions as the API serving the audio to these objects and handles the sound settings and sonic display implementation. Auditory icons (recognizable real-world sounds) and earcons (synthetic sounds) are used for game state awareness, while binaural processing (utilizing the KEMAR \gls{HRTF} library) conveys spatial details.

The game’s GUI is simple and employs intense contrast alongside significant optical elements (Figure~\ref{fig:tic-tac-toe-img}), accommodating users with residual sight. The auditory interface provides constant feedback on the effects of every action, and players sense direction and three-dimensionality through the localized sound cues. In addition, there are pre-recorded audio instructions for each game component.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{tic-tac-toe-cropped}
  \caption{Part of the Tic-Tac-Toe game's graphical user interface (Adapted from~\cite{drossos2015accessible}). \copyright~2015 ACM.}
  \label{fig:tic-tac-toe-img}
\end{figure}

In user studies, visually impaired children received the game very positively, though many were experiencing an accessible game for the first time. Drossos et al.’s findings are valuable for this dissertation as they address challenges this solution inevitably encountered, such as the need for constant, concurrent, and discernible audio feedback and conveying localization information through the sounds for users to navigate the soundscape. They also address relevant limitations, as not all sounds are equally effective for spatial localization and varying spatial awareness among users.


\subsection{The Preferred Spatial Awareness Tools for BVI People In Video Games}
\label{ssec:sats}

While the minimap is one of the most employed \gls{SAT}s in video games, crucial even for sighted players in learning the layout of their surroundings, it still has no successful equivalent regarding \gls{BVI} accessibility. Attempting to bridge this gap, Nair et al.~\cite{nair2021towards} took it upon themselves to tackle the creation of a universal and acoustic BVI-friendly minimap, or more concretely, uncovering the most relevant design factors as well as the merits and limitations of the best acoustic techniques to do so.

Two main questions were at the center of their study, the first regarding the key aspects of spatial awareness valued by visually impaired players in games and the second focusing on the effectiveness of current \gls{SAT}s in supporting said aspects.
Intending to delve into both, Nair et al.~\cite{nair2022uncovering} investigated the design of the four leading \gls{SAT}s (Figure~\ref{fig:dungeon-escape-img}) in enhancing spatial perception for the visually impaired in a \gls{3D} game world, vastly different approaches developed in previous research. These are the: 
\begin{itemize}
  \item \textit{Smartphone Map -} A touchscreen map working in tandem with the game to provide spatial information through sound effects and text-to-speech
  \item \textit{Whole-Room Shockwave –} When triggered, emits \gls{3D} sounds from certain objects based on their distance, simulating a refined and customizable echolocation (using Steam Audio\footnote{https://valvesoftware.github.io/steam-audio/index.html}'s built-in \gls{HRTF})
  \item \textit{Directional Scanner –} Allows players to survey a surrounding direction by tilting the right thumbstick towards it, announcing the first object in line-of-sight with directional sound
  \item \textit{Simple Audio Menu –} Lists the points of interest in a room through their corresponding sounds effects and text-to-speech, in alphabetical order
\end{itemize}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{dungeon-escape-cropped}
  \caption{The four spatial awareness tools implemented within Dungeon Escape~\cite{nair2022uncovering}). \copyright~2022 ACM.}
  \label{fig:dungeon-escape-img}
\end{figure}

To evaluate the effectiveness of each approach in conveying essential aspects of spatial awareness and address the two main questions of the research, Nair et al. implemented them all into Dungeon Escape. Dungeon Escape is an original third-person \gls{3D} game in which the players were required to use the given \gls{SAT}s to gain enough spatial awareness to succeed. Implemented in the Unity game engine, it is an adventure game specifically designed for the study. Players navigate dungeons in search of objects that allow them to overcome obstacles and thus escape. 

Despite its focus on studying the performance of the \gls{SAT}s within the differing dungeon layouts, it is more than a simple playground for the different \gls{SAT}s, portraying itself as an accessible game in other ways. As in well-known \gls{3D} games, the left thumbstick is used for movement and rotation, and in this case, aided by a utility mimicking snap rotation. Collisions have a unique sound effect, and relevant sound is played from any object within a 2-meter radius of the player. Furthermore, players can lock onto objects of interest by placing a looping audio beacon.

Following a user study with nine participants, eight of whom were completely blind, the most critical aspect of spatial awareness was found to be position and orientation. Presence, arrangement, and adjacent areas tied for second place, while shape and scale overwhelmingly came last. One of the key findings was that despite the importance of position and orientation, none of the approaches were entirely satisfactory, though the directional scanner performed the best. Another relevant takeaway was the effectiveness of combining some of the \gls{SAT}s with the most significant spatial awareness provided by the directional scanner and simple audio menu combination. Finally, visually impaired individuals greatly value customizable \gls{SAT}s, as the combination of the directional scanner and whole-room shockwave closely followed the aforementioned \gls{SAT} combination. This is despite the whole-room shockwave having been considered overwhelming in several instances (yet customizable) and the simple audio menu a great way to communicate presence (yet disliked by half the participants for being “spoilers”).

Nair et al.’s research provides several insights into the current dissertation, primarily by exposing the strengths and weaknesses of the different \gls{SAT}s and the accessible design of Dungeon Escape. Most importantly, it highlights the importance of position and orientation for proper spatial understanding and exploration, which was prioritized conveying in the current solution, possibly by incorporating combinations of the most synergetic \gls{SAT}s as was done in their study. Additionally, it touches on how users may be overwhelmed by too much detail, inspiring this research to focus on providing cues qualitatively rather than quantitatively.


\section{Tools for Soundscape Creation}

The current market is home to advanced soundscape creation software, enabling creators to design highly detailed and expressive auditory environments tailored to the most diverse applications. Different tools have different purposes, with some focusing on professional sound design, where the spatial accuracy of sound environments is the priority, whilst others aim for dynamism and adaptability.

Regarding sound design and immersive spatial audio, Dolby Atmos\footnote{https://www.dolby.com/technologies/dolby-atmos/} stands out as the industry’s standard in delivering \gls{3D} audio experiences, be it in entertainment, gaming, or \gls{VR}. In Dolby Atmos, environment creation is object-based, meaning that sounds may be placed and moved as discrete objects in a \gls{3D} space. Sound Particles\footnote{https://www.soundparticles.com/} is another tool meriting a mention, as it also delivers immersive, high-quality audio for films, gaming, and virtual environments, excelling on large-scale \gls{3D} audio effects by simulating spatialized sound with particle systems.

Though not exclusively centered around sound design and production, game engines such as Unity\footnote{https://unity.com/} and Unreal Engine\footnote{https://www.unrealengine.com/en-US} have proven effective platforms for soundscape creation, offering robust audio capabilities and integrations. Unity, for instance, provides built-in spatial sound support through its default audio system, with more advanced spatial audio capabilities being made available through integration with third-party tools like FMOD\footnote{https://www.fmod.com/}, Google’s Resonance Audio\footnote{https://resonance-audio.github.io/resonance-audio/} and Steam Audio\footnote{https://valvesoftware.github.io/steam-audio/}. Unreal Engine features a built-in audio engine supporting \gls{3D} spatialization, reverberation effects, and audio triggers, more advanced than Unity’s default audio system but not accounting for its third-party augmentations.

As expressive and competent as the tools mentioned so far are, they have a significant learning curve, a tradeoff for such feature richness and customization ability. In the context of this dissertation, where soundscape creation should not require prior experience in sound design, an adequate balance between expressiveness and complexity is a must. Additionally, there are specific design considerations which should be adhered to or at least considered regarding the visually impaired, addressed in specific research.

Guerreiro et al.~\cite{guerreiro2023design} proposed a theoretical framework to support BVI-inclusive auditory representations of object location, behavior, and interaction in virtual environments, utilizing spatialized sound and sonification in Unity. Their proposed design space explored nine distinct categories, such as the audio field, cardinality, concurrency, and spatialization. A user study they conducted showed that sound spatialization was not always preferred, mainly when dealing with moving objects, and that fully concurrent auditory feedback could be mentally overwhelming~\cite{guerreiro2023design}. Meanwhile, Krol et al.~\cite{krol2024design} investigated the use of automated musical soundscapes in visual art accessibility for the visually impaired. From their study, several design considerations were highlighted: enhancing narrative comprehension by integrating storytelling elements; soundscape customization options; integration with other accessibility methods such as audio descriptions; contextual and historical accuracy/adequacy; inclusion of ambient sound effects; effective audio reproduction such as spatial sound and reverb to convey location, movement, and atmosphere.


\subsection{Immerscape}
\label{ssec:immerscape}

The PASEV\footnote{https://www.uevora.pt/investigar/projetos?id=3776} project focuses on retaining and promoting the city of Évora’s cultural patrimony, namely the rich soundscapes correspondent to the various historical events which took place between 1540 and 1910. As part of this initiative, Ferreira~\cite{ferreira2021creating} proposed Immerscape.

Intended to provide the PASEV projects’s team with the means to reconstruct the city’s auditory history from current sound recordings, Immerscape is a soundscape editing tool accessible even to those without prior experience in programming or sound composition software. It allows for the creation of historical soundscapes out of previously collected recordings and the generation of immersive \gls{3D} audio files representing such soundscapes, while abstracting away the technical details behind spatial audio generation. The editor itself was implemented within the Unity game engine. The acoustic immersion leveraged the Google Resonance Audio SDK to spatialize sound by applying \gls{HRTF} filters, requiring headphones to be adequately evaluated.

Besides an accessible interface with minimal complexity (Figure~\ref{fig:immerscape-img}), Ferreira defined some fundamental requirements for Immerscape, such as the ability to select predefined \gls{3D} sound environments (with unique resonance and reverb properties), immersive environmental navigation, creation of editable sound sources (audio properties, movement and triggering events) and real-time playback/recording of the soundscape. Additionally, there are two available camera angles during development, the default being the player view and the alternative an up-view.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{immerscape-cropped}
  \caption{Immerscape's environment, through the player view perspective. The red cube represents an audio object (Reproduced from~\cite{ferreira2021creating}). \copyright~2021 Carolina Ribeiro Dias Ferreira.}
  \label{fig:immerscape-img}
\end{figure}

An evaluation with 10 developers and 7 non-developer participants highlighted Immerscape’s high usability and immersive quality, proven to be an effective way of providing engaging sonic experiences, in this case, by conveying historical soundscapes. 

While Ferreira’s work targets general audiences rather than the \gls{BVI} demographic, there is much to take away from it in the context of this dissertation.
Immerscape’s design and implementation of the \gls{HRTF} filters inspired what transpired throughout the current work, particularly regarding spatial audio simulation and ease of use for non-experienced users. Furthermore, some of the requirements defined by Ferreira, including editable audio sources and real-time playback of the scene, were imported into this dissertation's proposed editor, aligning with the goal of creating an intuitive yet expressive soundscape editor.
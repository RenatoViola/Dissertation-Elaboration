%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%

\chapter{Background}
\label{cha:background}

% epigraph configuration
\epigraphfontsize{\small\itshape}
\setlength\epigraphwidth{12.5cm}
\setlength\epigraphrule{0pt}

\epigraph{
  This chapter presents the foundational concepts most relevant to understanding the work to be developed. It covers sound principles from its definition, perception, transmission, and spatial localization. Additionally, the definition and purpose of soundscapes are addressed, and most importantly, the role these play in accessibility for the blind and visually impaired.
}{}

\glsresetall

\section{What Is Sound?}
\label{sec:what-is-sound}

As a physical phenomenon, sound is enabled by the vibration of a body with the properties of inertia and elasticity (which are attributes of nearly every object).

Any vibration can produce sound if it meets the requirements for moving a body back and forth. The simplest of vibrations can be characterized by a sinusoid (Figure~\ref{fig:sinusoid}) and is the elementary unit for all possible vibrations.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{sine_wave}
  \caption{Graphical representation of a simple sine wave (Adapted from~\cite{sinusoid-img}). \copyright~2010 - Bill Ansell.}
  \label{fig:sinusoid}
\end{figure}

Any vibration can be broken down into a composition of sine waves – a Fourier series, each uniquely identified by frequency, amplitude, and starting phase. Describing a complex vibration by deriving the characteristics of its composing simple vibrations is named a Fourier analysis~\cite{fundamentals-of-hearing-w-yost}.


\section{Sound Perception}
\label{sec:sound-perception}

Without delving into the anatomical details, hearing starts once a sound wave vibrates our eardrum. After passing through the outer, middle, and inner ear, what reaches our auditory nervous system is no longer a mechanical vibration but a nervous impulse, which our brain now interprets. 

Perceptually, the changes in amplitude of a sine wave tend to be experienced as loudness, while changes in frequency are labeled as pitch~\cite{fundamentals-of-hearing-w-yost}.


\section{Sound Propagation}
\label{sec:sound-propagation}

As mentioned in section~\ref{sec:what-is-sound}, for a sound to reach our ears or any other point, it must first travel through a medium with the properties of elasticity and inertia. 

That is to say that, for example, it can travel through solids, liquids, and gases but not through a vacuum~\cite{fundamentals-of-hearing-w-yost}. The speed at which it propagates may vary with the temperature and density of the medium, which in air is around 343 meters per second (at 20º C)~{\cite{risoud2018sound}}.

In the air, the very presence of its randomly moving molecules originates a static pressure, which, when disturbed by the vibrations of a sound source, leads to zones of alternating pressure (Figure~\ref{fig:pressure_zones} illustrates this) – where the molecules cluster more tightly is called an area of condensation. In contrast, a significant spread in molecule placement designates an area of rarefaction.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{pressure_zones}
  \caption{Representation of rarefaction and condensation zones (Adapted from~\cite{pressure-zones-img}).}
  \label{fig:pressure_zones}
\end{figure}

Sound waves propagate in all directions from a source (circularly in \gls{2D}, spherically in \gls{3D}) and, while traveling, may come across several forms of interference, such as reflection, absorption, diffraction, and refraction. For example, an obstacle with a size similar to that of the sound’s wavelength may produce an area past the object where wave magnitude is significantly reduced or even completely absent – a sound shadow.

In addition, the intensity of a sound decreases quadratically with the distance to its source - the inverse square law~\cite{fundamentals-of-hearing-w-yost}.


\section{Sound Localization}
\label{sec:sound-localization}

As sound has no intrinsic spatial dimensions, how we perceive spatial cues is a product of our auditory system’s capability to process the physical properties of sound that correspond to spatial position~\cite{fundamentals-of-hearing-w-yost}.

There exist three spatial dimensions in which one can localize sound: the horizontal plane, commonly referred to as the azimuth, the vertical plane (elevation), and the distance (range)~{\cite{risoud2018sound,fundamentals-of-hearing-w-yost}}. These are properly illustrated in Figure~\ref{fig:polar-sound-localization}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{polar-sound-localization}
  \caption{Polar coordinates used to locate a sound source in a \gls{3D} space centered on the listener (Adapted from~\cite{spatial-dimensions-img}).}
  \label{fig:polar-sound-localization}
\end{figure}

Most of our spatial perception heavily relies on our binaural hearing - our ability to interpret and locate what we hear from both ears. While monaural cues contribute to spatial hearing, namely for vertical localization and depth perception, the most important mechanisms for perceiving directional sound are binaural cues – used for localization in the horizontal plane, consisting of the interaural differences of time and intensity~\cite{spatial-audio-f-rumsey}. 

Via these differences in signal reception between both ears, time and level are key physical properties that enable sound localization along the azimuthal plane and will be further addressed in section~\ref{ssec:interaural-differences}.

Another important property is the sound’s spectral shape relevant for vertical localization, and it is impacted by interference phenomena such as reflection, diffraction, and absorption caused by a listener’s physical features~\cite{risoud2018sound}. These alterations are captured by a \gls{HRTF}, which we will delve into in section~\ref{ssec:hrtf}. 

Though not as much is known about perceiving the distance of a sound source, it is mainly influenced by the sound’s loudness and early reflections from nearby surfaces~\cite{fundamentals-of-hearing-w-yost}.


\subsection{Interaural Differences}
\label{ssec:interaural-differences}

The differences between the signals our two ears receive are coined as interaural differences. As briefly mentioned in section~\ref{sec:sound-localization}, they are our auditory system’s primary mechanisms for localizing sound along the azimuthal plane. Figure~\ref{fig:interaural-differences} illustrates them clearly.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{interaural-differences}
  \caption{Interaural Time Difference and Interaural Level Difference, respectively (Adapted from~\cite{interaural-differences-img}).}
  \label{fig:interaural-differences}
\end{figure}

These mechanisms depend on the sound signal's nature and can be influenced by environmental cues that introduce conflicting information~\cite{spatial-audio-f-rumsey}. One may classify interaural differences into two primary categories:
\begin{itemize}
  \item \textbf{Interaural Time Difference:} The \gls{ITD} refers to the delay in sound arriving in one ear compared to the other. 
  
  Binaural delay is the designation of the maximum time delay between the ears, and it enables the resolution of a source in the direction of the ear that first heard it – the precedence effect. The sound source’s angle of incidence affects the additional distance that the wave must travel to the farthest ear, thus impacting the binaural delay. The brain usually localizes the sound towards the earliest source for similar sound sources in different locations.
  
  When considering sinusoidal signals, \gls{ITD}s may be expressed as Interaural Phase Differences and are fundamental for locating low-frequency sounds where the wavelength is large enough for phase differences to be noticeable. For higher frequency sounds, the interpretation for both \gls{ITD} and \gls{IPD} becomes ambiguous~\cite{fundamentals-of-hearing-w-yost}, and one can no longer tell which ear is leading or lagging~\cite{risoud2018sound,spatial-audio-f-rumsey}.

  \item \textbf{Interaural Level Difference:} The \gls{ILD} pertains to the difference in intensity of the same sound between the two ears. 

  The intensity of the stimulus at the ear closer to the source is slightly more significant due to proximity (as explained by the inverse square law, mentioned at the end of section~\ref{sec:sound-propagation})~{\cite{spatial-audio-f-rumsey,fundamentals-of-hearing-w-yost}}. However, the extra distance the wave travels to the farther ear is negligible and the intensity differences minimal~\cite{spatial-audio-f-rumsey}.
 
  The primary contributor to \gls{ILD}s is not proximity but the attenuation caused by the head’s sound shadow for high-frequency sounds. A higher frequency implies a shorter wavelength and a greater sound shadow, thus a more noticeable difference in level~\cite{fundamentals-of-hearing-w-yost}. 
  The same cannot be said for low frequencies, at which the head is not a decent barrier to sound~\cite{spatial-audio-f-rumsey}.
\end{itemize}
These mechanisms complement each other in providing fundamental cues for localizing sound sources in the azimuthal plane. However, these are not without limitations, some of which we will address in subsection~\ref{ssec:confusion-cone}.

\subsection{The Cone Of Confusion}
\label{ssec:confusion-cone}

Though the interaural differences are crucial and effective in locating sound along the horizontal plane, one exception is the cone of confusion~\cite{risoud2018sound,fundamentals-of-hearing-w-yost}. 

When a listener’s head is stationary, detecting whether a sound is coming from the front or behind, or even its elevation, is far from obvious. The difficulty arises because some locations are producing the same interaural differences~\cite{risoud2018sound,fundamentals-of-hearing-w-yost}. The plane in which these ambiguous points lie is dubbed the mid-sagittal plane~\cite{fundamentals-of-hearing-w-yost}, and it forms the so-called cone of confusion (see Figure~\ref{fig:cone-of-confusion}), its axis being the interauricular line~\cite{risoud2018sound}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{cone-of-confusion}
  \caption{Cone of confusion, in which \gls{ITD}s and \gls{ILD}s are indistinguishable (Adapted from~\cite{cone-of-confusion-img}).}
  \label{fig:cone-of-confusion}
\end{figure}

Any sound deriving from this cone’s circumference produces neither time nor level differences, while any sound coming from elsewhere inside the cone has at least one other point mirroring its interaural differences~\cite{risoud2018sound}. Every sound source location has its cone of confusion describing the other possible locations producing the same interaural differences~\cite{fundamentals-of-hearing-w-yost}.

The most intuitive way to reduce this ambiguity is our natural head movements~\cite{risoud2018sound,spatial-audio-f-rumsey,fundamentals-of-hearing-w-yost}. Simply leaning our head or rotating it tears down the original cone of confusion by altering the \gls{ITD} and \gls{ILD} values, introducing additional binaural and spectral cues – enhancing localization. 

If head movement is not an option, our auditory system relies on the spectral cues derived from the \gls{HRTF}s~\cite{risoud2018sound,fundamentals-of-hearing-w-yost} (we will address them in some detail in section~\ref{ssec:hrtf}), which are not only critical in determining the vertical direction of sound but also in distinguishing between the ambiguous sounds along the azimuthal plane~\cite{risoud2018sound}.


\subsection{Head Related Transfer Function}
\label{ssec:hrtf}

As explained in section~\ref{ssec:confusion-cone}, due to the cone of confusion, interaural differences alone are not enough to adequately determine the direction of a sound, especially along the vertical plane.

The path of a sound to our ears is rarely uneventful, as our very anatomy induces significant changes in its spectrum. Our head, torso, and, more importantly, the structure of the pinna act as sound shadows in the sound’s trajectory, which are particularly accentuated for high-frequency sounds (since the wavelength is closer to the small size of the pinna)~{\cite{fundamentals-of-hearing-w-yost}}. Furthermore, the listed physical features interfere with an incoming sound wave through reflection, diffraction, and absorption, attenuating or delaying specific frequencies composing it~\cite{risoud2018sound,spatial-audio-f-rumsey}. 

These effects, caused by the filtering action of the pinna and body, are known as spectral cues~\cite{fundamentals-of-hearing-w-yost} and are determined monaurally~\cite{spatial-audio-f-rumsey}. Depending on both the position of the sound’s source as well as its angle of incidence~\cite{spatial-audio-f-rumsey,fundamentals-of-hearing-w-yost}, such cues uniquely shape a sound’s spectrum at the eardrum level. They are captured by what is called a Head Related Transfer Function (HRTF)~{\cite{fundamentals-of-hearing-w-yost}}. 

An \gls{HRTF} is a descriptor for the transformations a sound undertakes on its route to a listener’s tympanum due to the hearer’s physical features. 

For complex and high-frequency sounds, it is a highly competent mechanism for estimating the vertical position of a sound. However, it performs poorly for non-complex and low-frequency sounds (likely due to the wavelength exceeding the size of the pinna)~{\cite{risoud2018sound}}. Figure~\ref{fig:hrtf} shows how it may be measured.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{hrtf-2}
  \caption{\gls{HRTF} measurements in an anechoic chamber (Adapted from~\cite{hrtf-img}).}
  \label{fig:hrtf}
\end{figure}

Vertical localization is generally less accurate than its horizontal counterpart. However, coupling spectral cues with the previously mentioned interaural differences is a good approach for resolving the ambiguities within the cone of confusion without the need for head movement~\cite{fundamentals-of-hearing-w-yost}. 

Just as individuals greatly vary in their morphology, so does the shape and size of their pinna. Consequently, each person has a unique set of \gls{HRTF}s learned throughout their lives~\cite{risoud2018sound,spatial-audio-f-rumsey}.
Additionally, research has found that amplifications or attenuations in particular regions of the frequency spectrum seem to correspond to specific sound source positions~\cite{spatial-audio-f-rumsey}. 

Generalizing the spectral characteristics that compose \gls{HRTF}s across a vast demographic is convenient as it simplifies their implementation process while ensuring reasonable accuracy for many listeners. However, to no surprise, an individual’s best \gls{HRTF}s are their own~\cite{spatial-audio-f-rumsey}.

Nonetheless, there have been studies in which subjects are fed audio signals through \gls{HRTF}s other than their own~\cite{risoud2018sound,spatial-audio-f-rumsey} – to find their ability to localize sound significantly hampered~\cite{spatial-audio-f-rumsey}. After some time, the participants appear to begin learning the \gls{HRTF}s they have been given and eventually regain normal vertical localization~\cite{risoud2018sound}. Some people are known to localize audio better than others, so their \gls{HRTF}s tend to be considered more applicable for general use~\cite{spatial-audio-f-rumsey}.

Delivering audio stimuli over headphones is convenient and establishes a controlled environment\cite{fundamentals-of-hearing-w-yost}. However, certain headphone types induce a feeling of the sound emanating from inside the head, rather than the natural tridimensional externalization (azimuth, elevation, and range) we are accustomed to~\cite{spatial-audio-f-rumsey,fundamentals-of-hearing-w-yost}. 

In short, headphones fail to deliver on the nuances captured by \gls{HRTF}s since they emit sound directly to our ear, while a real outside source would first be affected by our physical features.

Luckily, simulating externalization is relatively intuitive: recreate a sound with all the spectral complexity of a real-world one by applying \gls{HRTF}-based filtering and only then transmit it~\cite{fundamentals-of-hearing-w-yost}.


\section{Soundscape}
\label{sec:soundscape}

By R.M. Schafer’s~\cite{schafer1993soundscape} definition, a soundscape englobes the various acoustic elements within an auditory environment in its totality and mainly focuses on how this environment is humanly interpreted~\cite{aletta2016soundscape,brown2011towards,dumyahn2011soundscape}. 

While a soundscape can be a physical phenomenon such as an acoustic environment, the emphasis on human perception distinguishes this term from being just that, as it can also be thought of as a perceptual concept~\cite{aletta2016soundscape,brown2011towards}. Its perceptual quality allows listeners to attribute meaning to their surroundings, immersing themselves personally. 

Interacting with a soundscape yields distinct results for different individuals, as people’s preferences and expectations greatly vary~\cite{dumyahn2011soundscape}, especially according to the space and context of the environment~\cite{brown2011towards}. 

Among other socially relevant values, soundscapes provide a sense of place and atmosphere, express cultural and historical values, and promote human connection to nature by enhancing the aestheticism of the experience. 

The fidelity of a soundscape is mainly evaluated on how different sound sources are perceived and the level of noise present~\cite{dumyahn2011soundscape}. So, too, is its general quality evaluated on several descriptors, some of which being noise annoyance, pleasantness, and quietness~\cite{aletta2016soundscape}. 

In the context of this dissertation, we are particularly interested in the role of the soundscape as an accessibility tool for \gls{BVI} individuals. Since their reliance on vision is limited, if any, they are not only more sensitive to auditory cues but can also draw more information from acoustic signals than sighted people. 

For these individuals, a high-fidelity soundscape is as functional as it is aesthetic since, from its acoustical information, they can derive information regarding the morphology and movement of objects. A fine example is rainfall, as it contours environmental elements, enhancing spatial awareness and connection to the environment~\cite{rychtarikova2012towards}.


\section{Discussion}
\label{sec:bg-discussion}

This chapter aims to provide a structured understanding of sound, building up to sound localization, the most critical aspect of this dissertation. We start by laying the groundwork through concise overviews of the most fundamental concepts regarding sound, such as what it is (section~\ref{sec:what-is-sound}), how humans perceive it (section~\ref{sec:sound-perception}) and how it travels (section~\ref{sec:sound-propagation}).

We then detail sound localization (section~\ref{sec:sound-localization}), introducing the three spatial dimensions where sound may be localized and exploring two techniques on which spatial audio heavily relies. Section~\ref{ssec:interaural-differences} covers the interaural differences of time and level, which are essential for the binaural determination of a sound source’s direction in the horizontal plane. Additionally, section~\ref{ssec:hrtf} explains the notion of \gls{HRTF}, its relevance for the monaural perception of \gls{3D} sound through spectral cues, and how it complements the interaural differences by resolving the cone of confusion (addressed in section~\ref{ssec:confusion-cone}). 

The chapter closes on a less technical note. Section~\ref{sec:soundscape} focuses on the soundscape as a means to evoke meaning and a sense of atmosphere within an auditory environment, briefly touching on how it is useful for \gls{BVI} individuals.

Chapter~\ref{cha:related-work} presents some studies related to our work where most, if not all, of the principles addressed were put into practice. In the research, we see the vast applications of sound within cultural settings and elsewhere, mainly studying its effectiveness in enhancing accessibility and engagement. \gls{3D} audio, interactive soundscapes and sound-based \gls{SAT}s are examples of such uses.

Informed by the implementation and findings of related studies, we incorporate the presented concepts into this thesis, with spatial audio standing at the core of our proposal. Employing the concepts of interaural differences and \gls{HRTF}s, we intend to non-visually convey the spatial characteristics of artworks through BVI-accessible interaction and exploration. In addition, we look to evoke emotional and aesthetic engagement beyond factual description, leveraging soundscape theory to this end.
